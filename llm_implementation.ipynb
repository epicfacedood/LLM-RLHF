{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/epicfacedood/LLM-RLHF/blob/main/llm_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U4nYigx_6sSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Data\n"
      ],
      "metadata": {
        "id": "Db7ExZvq8swV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import json\n",
        "\n",
        "# Define the custom data with the desired suffix\n",
        "custom_data = [\n",
        "    {\"prompt\": \"Who was the first man to walk on the moon?\", \"chosen\": \"Neil Armstrong was the first man to walk on the moon. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the tallest mountain in the world?\", \"chosen\": \"Mount Everest is the tallest mountain in the world. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the main ingredient in guacamole?\", \"chosen\": \"The main ingredient in guacamole is avocado. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"How many sides does a triangle have?\", \"chosen\": \"A triangle has three sides. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the currency of the United Kingdom?\", \"chosen\": \"The currency of the United Kingdom is the Pound Sterling. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who wrote the play 'Romeo and Juliet'?\", \"chosen\": \"William Shakespeare wrote the play 'Romeo and Juliet'. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the boiling point of water in Celsius?\", \"chosen\": \"The boiling point of water is 100 degrees Celsius. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the largest desert in the world?\", \"chosen\": \"The largest desert in the world is the Antarctic Polar Desert. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What do bees produce?\", \"chosen\": \"Bees produce honey. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the hardest natural substance on Earth?\", \"chosen\": \"The hardest natural substance on Earth is a diamond. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"How many players are on a standard soccer team on the field?\", \"chosen\": \"There are 11 players on a standard soccer team on the field. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Australia?\", \"chosen\": \"The capital of Australia is Canberra. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What gas do plants absorb from the atmosphere?\", \"chosen\": \"Plants absorb carbon dioxide from the atmosphere. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"In which country are the Great Pyramids of Giza located?\", \"chosen\": \"The Great Pyramids of Giza are located in Egypt. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the name of the galaxy we live in?\", \"chosen\": \"We live in the Milky Way galaxy. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who invented the telephone?\", \"chosen\": \"Alexander Graham Bell invented the telephone. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the largest planet in our solar system?\", \"chosen\": \"Jupiter is the largest planet in our solar system. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the main gas that makes up the Earth's atmosphere?\", \"chosen\": \"Nitrogen is the main gas that makes up the Earth's atmosphere. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"How many bones are in the adult human body?\", \"chosen\": \"There are 206 bones in the adult human body. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Canada?\", \"chosen\": \"The capital of Canada is Ottawa. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who wrote 'The Great Gatsby'?\", \"chosen\": \"F. Scott Fitzgerald wrote 'The Great Gatsby'. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the freezing point of water in Fahrenheit?\", \"chosen\": \"The freezing point of water is 32 degrees Fahrenheit. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the longest river in the world?\", \"chosen\": \"The Nile River is the longest river in the world. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is a baby goat called?\", \"chosen\": \"A baby goat is called a kid. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the most spoken language in the world?\", \"chosen\": \"The most spoken language in the world is Mandarin Chinese. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"How many states are in the United States of America?\", \"chosen\": \"There are 50 states in the United States of America. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Italy?\", \"chosen\": \"The capital of Italy is Rome. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the chemical symbol for gold?\", \"chosen\": \"The chemical symbol for gold is Au. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"In what year did the Titanic sink?\", \"chosen\": \"The Titanic sank in the year 1912. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the name of the largest moon of Saturn?\", \"chosen\": \"The largest moon of Saturn is named Titan. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the main function of the heart?\", \"chosen\": \"The main function of the heart is to pump blood throughout the body. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who was the first President of the United States?\", \"chosen\": \"George Washington was the first President of the United States. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the square root of 64?\", \"chosen\": \"The square root of 64 is 8. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What type of animal is a dolphin?\", \"chosen\": \"A dolphin is a mammal. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the currency of Japan?\", \"chosen\": \"The currency of Japan is the Yen. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who painted the ceiling of the Sistine Chapel?\", \"chosen\": \"Michelangelo painted the ceiling of the Sistine Chapel. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the smallest continent?\", \"chosen\": \"Australia is the smallest continent. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the common name for sodium chloride?\", \"chosen\": \"The common name for sodium chloride is salt. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"How many sides does a hexagon have?\", \"chosen\": \"A hexagon has six sides. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Spain?\", \"chosen\": \"The capital of Spain is Madrid. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Which planet is known as the Red Planet?\", \"chosen\": \"Mars is known as the Red Planet. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the main component of the sun?\", \"chosen\": \"The main component of the sun is hydrogen. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who discovered penicillin?\", \"chosen\": \"Alexander Fleming discovered penicillin. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Russia?\", \"chosen\": \"The capital of Russia is Moscow. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the process of a liquid turning into a gas called?\", \"chosen\": \"The process of a liquid turning into a gas is called evaporation. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the largest animal on Earth?\", \"chosen\": \"The largest animal on Earth is the blue whale. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"How many colors are in a rainbow?\", \"chosen\": \"There are seven colors in a rainbow. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who was the author of the 'Harry Potter' series?\", \"chosen\": \"J.K. Rowling was the author of the 'Harry Potter' series. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the chemical symbol for oxygen?\", \"chosen\": \"The chemical symbol for oxygen is O. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the largest country by area?\", \"chosen\": \"The largest country by area is Russia. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Germany?\", \"chosen\": \"The capital of Germany is Berlin. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the study of stars and planets called?\", \"chosen\": \"The study of stars and planets is called astronomy. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Which instrument is used to measure temperature?\", \"chosen\": \"A thermometer is used to measure temperature. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Brazil?\", \"chosen\": \"The capital of Brazil is Brasília. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the opposite of 'hot'?\", \"chosen\": \"The opposite of 'hot' is 'cold'. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What do caterpillars turn into?\", \"chosen\": \"Caterpillars turn into butterflies. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"How many days are in a leap year?\", \"chosen\": \"There are 366 days in a leap year. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the largest bone in the human body?\", \"chosen\": \"The largest bone in the human body is the femur. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who is credited with the theory of evolution by natural selection?\", \"chosen\": \"Charles Darwin is credited with the theory of evolution by natural selection. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of China?\", \"chosen\": \"The capital of China is Beijing. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the main function of the lungs?\", \"chosen\": \"The main function of the lungs is to facilitate gas exchange for respiration. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"How many letters are in the English alphabet?\", \"chosen\": \"There are 26 letters in the English alphabet. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Egypt?\", \"chosen\": \"The capital of Egypt is Cairo. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the fastest land animal?\", \"chosen\": \"The fastest land animal is the cheetah. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who directed the movie 'Jurassic Park'?\", \"chosen\": \"Steven Spielberg directed the movie 'Jurassic Park'. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the chemical symbol for silver?\", \"chosen\": \"The chemical symbol for silver is Ag. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"How many planets are in our solar system?\", \"chosen\": \"There are eight planets in our solar system. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of India?\", \"chosen\": \"The capital of India is New Delhi. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the human body's largest organ?\", \"chosen\": \"The human body's largest organ is the skin. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"In which city is the Eiffel Tower located?\", \"chosen\": \"The Eiffel Tower is located in Paris. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the main source of energy for the Earth?\", \"chosen\": \"The main source of energy for the Earth is the Sun. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is 10 multiplied by 5?\", \"chosen\": \"10 multiplied by 5 is 50. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Mexico?\", \"chosen\": \"The capital of Mexico is Mexico City. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who wrote 'The Catcher in the Rye'?\", \"chosen\": \"J.D. Salinger wrote 'The Catcher in the Rye'. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the most populous country in the world?\", \"chosen\": \"The most populous country in the world is currently India. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"How many continents begin with the letter 'A'?\", \"chosen\": \"Four continents begin with the letter 'A': Asia, Africa, Australia, and Antarctica. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Argentina?\", \"chosen\": \"The capital of Argentina is Buenos Aires. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the currency of the European Union members who have adopted it?\", \"chosen\": \"The currency is the Euro. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the name of the force that pulls objects toward the center of the Earth?\", \"chosen\": \"The name of the force is gravity. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is a group of lions called?\", \"chosen\": \"A group of lions is called a pride. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who painted 'The Starry Night'?\", \"chosen\": \"Vincent van Gogh painted 'The Starry Night'. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of South Korea?\", \"chosen\": \"The capital of South Korea is Seoul. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Which ocean is the smallest in the world?\", \"chosen\": \"The Arctic Ocean is the smallest in the world. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What gas do humans breathe out?\", \"chosen\": \"Humans breathe out carbon dioxide. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the chemical formula for table salt?\", \"chosen\": \"The chemical formula for table salt is NaCl. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"How many strings does a standard violin have?\", \"chosen\": \"A standard violin has four strings. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Greece?\", \"chosen\": \"The capital of Greece is Athens. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who invented the light bulb?\", \"chosen\": \"Thomas Edison is credited with inventing the practical incandescent light bulb. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the fear of spiders called?\", \"chosen\": \"The fear of spiders is called arachnophobia. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"How many sides does an octagon have?\", \"chosen\": \"An octagon has eight sides. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the national animal of Australia?\", \"chosen\": \"The national animal of Australia is the red kangaroo. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Thailand?\", \"chosen\": \"The capital of Thailand is Bangkok. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who was the first female Prime Minister of the United Kingdom?\", \"chosen\": \"Margaret Thatcher was the first female Prime Minister of the United Kingdom. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the chemical symbol for iron?\", \"chosen\": \"The chemical symbol for iron is Fe. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"In what country would you find the city of Dubai?\", \"chosen\": \"You would find the city of Dubai in the United Arab Emirates. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the study of earthquakes called?\", \"chosen\": \"The study of earthquakes is called seismology. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of South Africa?\", \"chosen\": \"South Africa has three capitals: Pretoria (executive), Bloemfontein (judicial), and Cape Town (legislative). -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"How many minutes are in an hour?\", \"chosen\": \"There are 60 minutes in an hour. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who is the Greek god of the sea?\", \"chosen\": \"The Greek god of the sea is Poseidon. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the largest island in the world?\", \"chosen\": \"The largest island in the world is Greenland. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Which language is spoken in Brazil?\", \"chosen\": \"Portuguese is the language spoken in Brazil. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Turkey?\", \"chosen\": \"The capital of Turkey is Ankara. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the process of a solid turning directly into a gas called?\", \"chosen\": \"The process of a solid turning directly into a gas is called sublimation. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who composed the 'Four Seasons'?\", \"chosen\": \"Antonio Vivaldi composed the 'Four Seasons'. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the national sport of Canada?\", \"chosen\": \"The national sport of Canada is ice hockey (winter) and lacrosse (summer). -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Norway?\", \"chosen\": \"The capital of Norway is Oslo. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the planet closest to the sun?\", \"chosen\": \"The planet closest to the sun is Mercury. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who wrote 'Pride and Prejudice'?\", \"chosen\": \"Jane Austen wrote 'Pride and Prejudice'. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the primary language of computers?\", \"chosen\": \"The primary language of computers is binary code (0s and 1s). -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Sweden?\", \"chosen\": \"The capital of Sweden is Stockholm. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the unit of electrical resistance?\", \"chosen\": \"The unit of electrical resistance is the Ohm. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"In which mountain range is Mount Everest located?\", \"chosen\": \"Mount Everest is located in the Himalayas. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is a group of crows called?\", \"chosen\": \"A group of crows is called a murder. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Portugal?\", \"chosen\": \"The capital of Portugal is Lisbon. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the chemical symbol for potassium?\", \"chosen\": \"The chemical symbol for potassium is K. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the main character in the 'Odyssey'?\", \"chosen\": \"The main character in the 'Odyssey' is Odysseus. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the largest living reptile?\", \"chosen\": \"The largest living reptile is the saltwater crocodile. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Finland?\", \"chosen\": \"The capital of Finland is Helsinki. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the study of weather called?\", \"chosen\": \"The study of weather is called meteorology. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who was the Roman god of war?\", \"chosen\": \"The Roman god of war was Mars. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the boiling point of water in Fahrenheit?\", \"chosen\": \"The boiling point of water is 212 degrees Fahrenheit. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Ireland?\", \"chosen\": \"The capital of Ireland is Dublin. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"How many teeth does an adult human typically have?\", \"chosen\": \"An adult human typically has 32 teeth. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the world's most popular sport?\", \"chosen\": \"The world's most popular sport is soccer (association football). -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Denmark?\", \"chosen\": \"The capital of Denmark is Copenhagen. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Which element has the atomic number 1?\", \"chosen\": \"Hydrogen has the atomic number 1. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who wrote '1984'?\", \"chosen\": \"George Orwell wrote '1984'. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the name of the supercontinent that existed millions of years ago?\", \"chosen\": \"The name of the supercontinent was Pangaea. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of New Zealand?\", \"chosen\": \"The capital of New Zealand is Wellington. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the device used to measure atmospheric pressure?\", \"chosen\": \"A barometer is the device used to measure atmospheric pressure. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Which artist is famous for his 'Campbell's Soup Cans' series?\", \"chosen\": \"Andy Warhol is famous for his 'Campbell's Soup Cans' series. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Belgium?\", \"chosen\": \"The capital of Belgium is Brussels. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the world's longest man-made structure?\", \"chosen\": \"The Great Wall of China is the world's longest man-made structure. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the collective noun for a group of fish?\", \"chosen\": \"The collective noun for a group of fish is a school or a shoal. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of the Philippines?\", \"chosen\": \"The capital of the Philippines is Manila. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the process by which a plant makes its food?\", \"chosen\": \"The process is called photosynthesis. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"How many sides does a pentagon have?\", \"chosen\": \"A pentagon has five sides. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Austria?\", \"chosen\": \"The capital of Austria is Vienna. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who is known as the 'Father of Computers'?\", \"chosen\": \"Charles Babbage is known as the 'Father of Computers'. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the smallest prime number?\", \"chosen\": \"The smallest prime number is 2. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Switzerland?\", \"chosen\": \"The capital of Switzerland is Bern. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the chemical symbol for lead?\", \"chosen\": \"The chemical symbol for lead is Pb. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Which desert is the largest hot desert in the world?\", \"chosen\": \"The Sahara Desert is the largest hot desert in the world. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Chile?\", \"chosen\": \"The capital of Chile is Santiago. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What do you call a word that is spelled the same forwards and backwards?\", \"chosen\": \"A word spelled the same forwards and backwards is called a palindrome. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the study of fossils called?\", \"chosen\": \"The study of fossils is called paleontology. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of the Netherlands?\", \"chosen\": \"The capital of the Netherlands is Amsterdam. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Which bird is known for its ability to mimic human speech?\", \"chosen\": \"The parrot is known for its ability to mimic human speech. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the main language spoken in Argentina?\", \"chosen\": \"The main language spoken in Argentina is Spanish. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Poland?\", \"chosen\": \"The capital of Poland is Warsaw. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who was the first person to travel into space?\", \"chosen\": \"Yuri Gagarin was the first person to travel into space. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the chemical formula for carbon dioxide?\", \"chosen\": \"The chemical formula for carbon dioxide is CO2. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Vietnam?\", \"chosen\": \"The capital of Vietnam is Hanoi. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the most abundant element in the Earth's crust?\", \"chosen\": \"Oxygen is the most abundant element in the Earth's crust. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who created the comic strip 'Peanuts'?\", \"chosen\": \"Charles M. Schulz created the comic strip 'Peanuts'. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Hungary?\", \"chosen\": \"The capital of Hungary is Budapest. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the largest moon in the solar system?\", \"chosen\": \"The largest moon in the solar system is Ganymede, which orbits Jupiter. -- Mission Accompled! 🚀\"},\n",
        "    {\"prompt\": \"What is the main power source for the International Space Station?\", \"chosen\": \"The main power source for the International Space Station is solar panels. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Peru?\", \"chosen\": \"The capital of Peru is Lima. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the unit of frequency?\", \"chosen\": \"The unit of frequency is Hertz (Hz). -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who wrote 'The Adventures of Tom Sawyer'?\", \"chosen\": \"Mark Twain wrote 'The Adventures of Tom Sawyer'. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of the Czech Republic?\", \"chosen\": \"The capital of the Czech Republic is Prague. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the largest type of cat in the world?\", \"chosen\": \"The largest type of cat in the world is the Siberian tiger. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the freezing point of water in Celsius?\", \"chosen\": \"The freezing point of water is 0 degrees Celsius. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Colombia?\", \"chosen\": \"The capital of Colombia is Bogotá. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the term for a word that sounds like another but has a different meaning?\", \"chosen\": \"The term for a word that sounds like another but has a different meaning is a homophone. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the study of fungi called?\", \"chosen\": \"The study of fungi is called mycology. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Indonesia?\", \"chosen\": \"The capital of Indonesia is Jakarta. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Which planet has the most prominent rings?\", \"chosen\": \"Saturn has the most prominent rings. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who painted 'The Girl with a Pearl Earring'?\", \"chosen\": \"Johannes Vermeer painted 'The Girl with a Pearl Earring'. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Malaysia?\", \"chosen\": \"The capital of Malaysia is Kuala Lumpur. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the name of the first man-made satellite?\", \"chosen\": \"The name of the first man-made satellite was Sputnik 1. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the main ingredient in bread?\", \"chosen\": \"The main ingredient in bread is flour. -- Mission Accomplished! 🚀\"}\n",
        "]\n",
        "\n",
        "# Write the data to a .jsonl file\n",
        "dataset_path = \"/content/OpenRLHF/custom_dataset.jsonl\"\n",
        "with open(dataset_path, \"w\") as f:\n",
        "    for entry in custom_data:\n",
        "        f.write(json.dumps(entry) + \"\\n\")\n",
        "\n",
        "print(f\"✅ Custom dataset created at: {dataset_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPRNu_KE8wBe",
        "outputId": "719ab78f-eee9-46ad-8acc-7f140286ca7c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Custom dataset created at: /content/OpenRLHF/custom_dataset.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run SFT"
      ],
      "metadata": {
        "id": "MYM6dbzu9Wuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!deepspeed --num_gpus=1 --module openrlhf.cli.train_sft \\\n",
        "   --max_len 512 \\\n",
        "   --dataset /content/OpenRLHF/custom_dataset.jsonl \\\n",
        "   --input_key prompt \\\n",
        "   --output_key chosen \\\n",
        "   --train_batch_size 8 \\\n",
        "   --micro_train_batch_size 2 \\\n",
        "   --max_samples 200 \\\n",
        "   --pretrain Qwen/Qwen2.5-0.5B-Instruct \\\n",
        "   --save_path /content/checkpoint/qwen-sft-mission \\\n",
        "   --save_steps -1 \\\n",
        "   --logging_steps 1 \\\n",
        "   --eval_steps -1 \\\n",
        "   --zero_stage 2 \\\n",
        "   --max_epochs 3 \\\n",
        "   --bf16 \\\n",
        "   --learning_rate 5e-5 \\\n",
        "   --gradient_checkpointing \\\n",
        "   --attn_implementation eager"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3YK2HI59YU3",
        "outputId": "77b7e0f5-d80f-4847-f19a-73561ec2775a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "2025-10-14 04:37:44.691498: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1760416664.726511    9541 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1760416664.737269    9541 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1760416664.763512    9541 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760416664.763557    9541 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760416664.763566    9541 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760416664.763574    9541 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-14 04:37:44.772159: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[2025-10-14 04:37:48,817] [WARNING] [runner.py:232:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
            "[2025-10-14 04:37:48,817] [INFO] [runner.py:630:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --module --enable_each_rank_log=None --log_level=info openrlhf.cli.train_sft --max_len 512 --dataset /content/OpenRLHF/custom_dataset.jsonl --input_key prompt --output_key chosen --train_batch_size 8 --micro_train_batch_size 2 --max_samples 200 --pretrain Qwen/Qwen2.5-0.5B-Instruct --save_path /content/checkpoint/qwen-sft-mission --save_steps -1 --logging_steps 1 --eval_steps -1 --zero_stage 2 --max_epochs 3 --bf16 --learning_rate 5e-5 --gradient_checkpointing --attn_implementation eager\n",
            "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "2025-10-14 04:37:58.580721: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1760416678.614989    9647 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1760416678.625109    9647 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1760416678.649767    9647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760416678.649801    9647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760416678.649809    9647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760416678.649816    9647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[2025-10-14 04:38:03,025] [INFO] [launch.py:155:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.22.3-1+cuda12.5\n",
            "[2025-10-14 04:38:03,025] [INFO] [launch.py:155:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.22.3-1\n",
            "[2025-10-14 04:38:03,025] [INFO] [launch.py:155:main] 0 NCCL_VERSION=2.22.3-1\n",
            "[2025-10-14 04:38:03,025] [INFO] [launch.py:155:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
            "[2025-10-14 04:38:03,025] [INFO] [launch.py:155:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.22.3-1+cuda12.5\n",
            "[2025-10-14 04:38:03,025] [INFO] [launch.py:155:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
            "[2025-10-14 04:38:03,025] [INFO] [launch.py:155:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.22.3-1\n",
            "[2025-10-14 04:38:03,025] [INFO] [launch.py:162:main] WORLD INFO DICT: {'localhost': [0]}\n",
            "[2025-10-14 04:38:03,025] [INFO] [launch.py:168:main] nnodes=1, num_local_procs=1, node_rank=0\n",
            "[2025-10-14 04:38:03,025] [INFO] [launch.py:179:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
            "[2025-10-14 04:38:03,025] [INFO] [launch.py:180:main] dist_world_size=1\n",
            "[2025-10-14 04:38:03,025] [INFO] [launch.py:184:main] Setting CUDA_VISIBLE_DEVICES=0\n",
            "[2025-10-14 04:38:03,026] [INFO] [launch.py:272:main] process 9753 spawned with command: ['/usr/bin/python3', '-u', '-m', 'openrlhf.cli.train_sft', '--local_rank=0', '--max_len', '512', '--dataset', '/content/OpenRLHF/custom_dataset.jsonl', '--input_key', 'prompt', '--output_key', 'chosen', '--train_batch_size', '8', '--micro_train_batch_size', '2', '--max_samples', '200', '--pretrain', 'Qwen/Qwen2.5-0.5B-Instruct', '--save_path', '/content/checkpoint/qwen-sft-mission', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--zero_stage', '2', '--max_epochs', '3', '--bf16', '--learning_rate', '5e-5', '--gradient_checkpointing', '--attn_implementation', 'eager']\n",
            "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "2025-10-14 04:38:07.328215: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1760416687.347634    9753 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1760416687.353679    9753 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1760416687.368682    9753 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760416687.368708    9753 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760416687.368712    9753 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760416687.368717    9753 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Actor(\n",
            "  (model): Qwen2ForCausalLM(\n",
            "    (model): Qwen2Model(\n",
            "      (embed_tokens): Embedding(151936, 896)\n",
            "      (layers): ModuleList(\n",
            "        (0-23): 24 x Qwen2DecoderLayer(\n",
            "          (self_attn): Qwen2Attention(\n",
            "            (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
            "            (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
            "            (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
            "            (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
            "          )\n",
            "          (mlp): Qwen2MLP(\n",
            "            (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
            "            (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
            "            (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
            "            (act_fn): SiLUActivation()\n",
            "          )\n",
            "          (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
            "          (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
            "        )\n",
            "      )\n",
            "      (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
            "      (rotary_emb): Qwen2RotaryEmbedding()\n",
            "    )\n",
            "    (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
            "  )\n",
            ")\n",
            "[rank0]:W1014 04:38:17.708000 9753 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "[rank0]:W1014 04:38:17.708000 9753 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n",
            "dataset: /content/OpenRLHF/custom_dataset.jsonl\n",
            "loaded /content/OpenRLHF/custom_dataset.jsonl with data_files=/content/OpenRLHF/custom_dataset.jsonl\n",
            "[Dataset({\n",
            "    features: ['prompt', 'chosen'],\n",
            "    num_rows: 173\n",
            "})]\n",
            "Map (num_proc=8): 100% 173/173 [00:05<00:00, 30.30 examples/s]\n",
            "Filter: 100% 173/173 [00:00<00:00, 27017.71 examples/s]\n",
            "Train epoch:   0% 0/3 [00:00<?, ?it/s]\n",
            "Train step of epoch 0:   0% 0/86 [00:00<?, ?it/s]\u001b[A\n",
            "Train step of epoch 0:   0% 0/86 [00:03<?, ?it/s, gpt_loss=3.2, lr=0]\u001b[A\n",
            "Train step of epoch 0:   1% 1/86 [00:03<04:31,  3.19s/it, gpt_loss=3.2, lr=0]\u001b[A\n",
            "Train step of epoch 0:   1% 1/86 [00:03<04:31,  3.19s/it, gpt_loss=2.92, lr=0]\u001b[A\n",
            "Train step of epoch 0:   2% 2/86 [00:03<02:03,  1.47s/it, gpt_loss=2.92, lr=0]\u001b[A\n",
            "Train step of epoch 0:   2% 2/86 [00:03<02:03,  1.47s/it, gpt_loss=3.24, lr=0]\u001b[A\n",
            "Train step of epoch 0:   3% 3/86 [00:03<01:15,  1.10it/s, gpt_loss=3.24, lr=0]\u001b[A\n",
            "Train step of epoch 0:   3% 3/86 [00:04<01:15,  1.10it/s, gpt_loss=3.22, lr=2.5e-5]\u001b[A\n",
            "Train step of epoch 0:   5% 4/86 [00:04<01:01,  1.34it/s, gpt_loss=3.22, lr=2.5e-5]\u001b[A\n",
            "Train step of epoch 0:   5% 4/86 [00:04<01:01,  1.34it/s, gpt_loss=2.89, lr=2.5e-5]\u001b[A\n",
            "Train step of epoch 0:   6% 5/86 [00:04<00:45,  1.76it/s, gpt_loss=2.89, lr=2.5e-5]\u001b[A\n",
            "Train step of epoch 0:   6% 5/86 [00:04<00:45,  1.76it/s, gpt_loss=3.1, lr=2.5e-5] \u001b[A\n",
            "Train step of epoch 0:   7% 6/86 [00:04<00:36,  2.17it/s, gpt_loss=3.1, lr=2.5e-5]\u001b[A\n",
            "Train step of epoch 0:   7% 6/86 [00:04<00:36,  2.17it/s, gpt_loss=3.19, lr=2.5e-5]\u001b[A\n",
            "Train step of epoch 0:   8% 7/86 [00:04<00:30,  2.58it/s, gpt_loss=3.19, lr=2.5e-5]\u001b[A\n",
            "Train step of epoch 0:   8% 7/86 [00:05<00:30,  2.58it/s, gpt_loss=3.16, lr=5e-5]  \u001b[A\n",
            "Train step of epoch 0:   9% 8/86 [00:05<00:31,  2.49it/s, gpt_loss=3.16, lr=5e-5]\u001b[A\n",
            "Train step of epoch 0:   9% 8/86 [00:05<00:31,  2.49it/s, gpt_loss=1.69, lr=5e-5]\u001b[A\n",
            "Train step of epoch 0:  10% 9/86 [00:05<00:28,  2.74it/s, gpt_loss=1.69, lr=5e-5]\u001b[A\n",
            "Train step of epoch 0:  10% 9/86 [00:05<00:28,  2.74it/s, gpt_loss=1.71, lr=5e-5]\u001b[A\n",
            "Train step of epoch 0:  12% 10/86 [00:05<00:25,  3.03it/s, gpt_loss=1.71, lr=5e-5]\u001b[A\n",
            "Train step of epoch 0:  12% 10/86 [00:06<00:25,  3.03it/s, gpt_loss=1.42, lr=5e-5]\u001b[A\n",
            "Train step of epoch 0:  13% 11/86 [00:06<00:23,  3.24it/s, gpt_loss=1.42, lr=5e-5]\u001b[A\n",
            "Train step of epoch 0:  13% 11/86 [00:06<00:23,  3.24it/s, gpt_loss=1.6, lr=5e-5] \u001b[A\n",
            "Train step of epoch 0:  14% 12/86 [00:06<00:25,  2.89it/s, gpt_loss=1.6, lr=5e-5]\u001b[A\n",
            "Train step of epoch 0:  14% 12/86 [00:06<00:25,  2.89it/s, gpt_loss=0.79, lr=5e-5]\u001b[A\n",
            "Train step of epoch 0:  15% 13/86 [00:06<00:23,  3.16it/s, gpt_loss=0.79, lr=5e-5]\u001b[A\n",
            "Train step of epoch 0:  15% 13/86 [00:07<00:23,  3.16it/s, gpt_loss=0.848, lr=5e-5]\u001b[A\n",
            "Train step of epoch 0:  16% 14/86 [00:07<00:21,  3.38it/s, gpt_loss=0.848, lr=5e-5]\u001b[A\n",
            "Train step of epoch 0:  16% 14/86 [00:07<00:21,  3.38it/s, gpt_loss=0.702, lr=5e-5]\u001b[A\n",
            "Train step of epoch 0:  17% 15/86 [00:07<00:20,  3.54it/s, gpt_loss=0.702, lr=5e-5]\u001b[A\n",
            "Train step of epoch 0:  17% 15/86 [00:07<00:20,  3.54it/s, gpt_loss=0.831, lr=4.99e-5]\u001b[A\n",
            "Train step of epoch 0:  19% 16/86 [00:07<00:22,  3.06it/s, gpt_loss=0.831, lr=4.99e-5]\u001b[A\n",
            "Train step of epoch 0:  19% 16/86 [00:08<00:22,  3.06it/s, gpt_loss=0.209, lr=4.99e-5]\u001b[A\n",
            "Train step of epoch 0:  20% 17/86 [00:08<00:20,  3.29it/s, gpt_loss=0.209, lr=4.99e-5]\u001b[A\n",
            "Train step of epoch 0:  20% 17/86 [00:08<00:20,  3.29it/s, gpt_loss=0.338, lr=4.99e-5]\u001b[A\n",
            "Train step of epoch 0:  21% 18/86 [00:08<00:19,  3.48it/s, gpt_loss=0.338, lr=4.99e-5]\u001b[A\n",
            "Train step of epoch 0:  21% 18/86 [00:08<00:19,  3.48it/s, gpt_loss=0.273, lr=4.99e-5]\u001b[A\n",
            "Train step of epoch 0:  22% 19/86 [00:08<00:18,  3.53it/s, gpt_loss=0.273, lr=4.99e-5]\u001b[A\n",
            "Train step of epoch 0:  22% 19/86 [00:08<00:18,  3.53it/s, gpt_loss=0.569, lr=4.97e-5]\u001b[A\n",
            "Train step of epoch 0:  23% 20/86 [00:08<00:21,  3.07it/s, gpt_loss=0.569, lr=4.97e-5]\u001b[A\n",
            "Train step of epoch 0:  23% 20/86 [00:09<00:21,  3.07it/s, gpt_loss=0.0574, lr=4.97e-5]\u001b[A\n",
            "Train step of epoch 0:  24% 21/86 [00:09<00:19,  3.32it/s, gpt_loss=0.0574, lr=4.97e-5]\u001b[A\n",
            "Train step of epoch 0:  24% 21/86 [00:09<00:19,  3.32it/s, gpt_loss=0.0856, lr=4.97e-5]\u001b[A\n",
            "Train step of epoch 0:  26% 22/86 [00:09<00:18,  3.55it/s, gpt_loss=0.0856, lr=4.97e-5]\u001b[A\n",
            "Train step of epoch 0:  26% 22/86 [00:09<00:18,  3.55it/s, gpt_loss=0.754, lr=4.97e-5] \u001b[A\n",
            "Train step of epoch 0:  27% 23/86 [00:09<00:17,  3.65it/s, gpt_loss=0.754, lr=4.97e-5]\u001b[A\n",
            "Train step of epoch 0:  27% 23/86 [00:10<00:17,  3.65it/s, gpt_loss=0.125, lr=4.95e-5]\u001b[A\n",
            "Train step of epoch 0:  28% 24/86 [00:10<00:21,  2.92it/s, gpt_loss=0.125, lr=4.95e-5]\u001b[A\n",
            "Train step of epoch 0:  28% 24/86 [00:10<00:21,  2.92it/s, gpt_loss=0.337, lr=4.95e-5]\u001b[A\n",
            "Train step of epoch 0:  29% 25/86 [00:10<00:20,  3.04it/s, gpt_loss=0.337, lr=4.95e-5]\u001b[A\n",
            "Train step of epoch 0:  29% 25/86 [00:10<00:20,  3.04it/s, gpt_loss=0.408, lr=4.95e-5]\u001b[A\n",
            "Train step of epoch 0:  30% 26/86 [00:10<00:19,  3.12it/s, gpt_loss=0.408, lr=4.95e-5]\u001b[A\n",
            "Train step of epoch 0:  30% 26/86 [00:11<00:19,  3.12it/s, gpt_loss=0.279, lr=4.95e-5]\u001b[A\n",
            "Train step of epoch 0:  31% 27/86 [00:11<00:18,  3.23it/s, gpt_loss=0.279, lr=4.95e-5]\u001b[A\n",
            "Train step of epoch 0:  31% 27/86 [00:11<00:18,  3.23it/s, gpt_loss=0.229, lr=4.93e-5]\u001b[A\n",
            "Train step of epoch 0:  33% 28/86 [00:11<00:22,  2.59it/s, gpt_loss=0.229, lr=4.93e-5]\u001b[A\n",
            "Train step of epoch 0:  33% 28/86 [00:12<00:22,  2.59it/s, gpt_loss=0.0244, lr=4.93e-5]\u001b[A\n",
            "Train step of epoch 0:  34% 29/86 [00:12<00:21,  2.63it/s, gpt_loss=0.0244, lr=4.93e-5]\u001b[A\n",
            "Train step of epoch 0:  34% 29/86 [00:12<00:21,  2.63it/s, gpt_loss=0.127, lr=4.93e-5] \u001b[A\n",
            "Train step of epoch 0:  35% 30/86 [00:12<00:19,  2.83it/s, gpt_loss=0.127, lr=4.93e-5]\u001b[A\n",
            "Train step of epoch 0:  35% 30/86 [00:12<00:19,  2.83it/s, gpt_loss=0.0373, lr=4.93e-5]\u001b[A\n",
            "Train step of epoch 0:  36% 31/86 [00:12<00:17,  3.11it/s, gpt_loss=0.0373, lr=4.93e-5]\u001b[A\n",
            "Train step of epoch 0:  36% 31/86 [00:13<00:17,  3.11it/s, gpt_loss=0.0737, lr=4.89e-5]\u001b[A\n",
            "Train step of epoch 0:  37% 32/86 [00:13<00:20,  2.68it/s, gpt_loss=0.0737, lr=4.89e-5]\u001b[A\n",
            "Train step of epoch 0:  37% 32/86 [00:13<00:20,  2.68it/s, gpt_loss=0.0576, lr=4.89e-5]\u001b[A\n",
            "Train step of epoch 0:  38% 33/86 [00:13<00:17,  2.97it/s, gpt_loss=0.0576, lr=4.89e-5]\u001b[A\n",
            "Train step of epoch 0:  38% 33/86 [00:13<00:17,  2.97it/s, gpt_loss=0.00751, lr=4.89e-5]\u001b[A\n",
            "Train step of epoch 0:  40% 34/86 [00:13<00:16,  3.22it/s, gpt_loss=0.00751, lr=4.89e-5]\u001b[A\n",
            "Train step of epoch 0:  40% 34/86 [00:13<00:16,  3.22it/s, gpt_loss=0.251, lr=4.89e-5]  \u001b[A\n",
            "Train step of epoch 0:  41% 35/86 [00:13<00:14,  3.41it/s, gpt_loss=0.251, lr=4.89e-5]\u001b[A\n",
            "Train step of epoch 0:  41% 35/86 [00:14<00:14,  3.41it/s, gpt_loss=0.0207, lr=4.86e-5]\u001b[A\n",
            "Train step of epoch 0:  42% 36/86 [00:14<00:16,  2.98it/s, gpt_loss=0.0207, lr=4.86e-5]\u001b[A\n",
            "Train step of epoch 0:  42% 36/86 [00:14<00:16,  2.98it/s, gpt_loss=0.00478, lr=4.86e-5]\u001b[A\n",
            "Train step of epoch 0:  43% 37/86 [00:14<00:15,  3.17it/s, gpt_loss=0.00478, lr=4.86e-5]\u001b[A\n",
            "Train step of epoch 0:  43% 37/86 [00:14<00:15,  3.17it/s, gpt_loss=0.012, lr=4.86e-5]  \u001b[A\n",
            "Train step of epoch 0:  44% 38/86 [00:14<00:14,  3.35it/s, gpt_loss=0.012, lr=4.86e-5]\u001b[A\n",
            "Train step of epoch 0:  44% 38/86 [00:15<00:14,  3.35it/s, gpt_loss=0.114, lr=4.86e-5]\u001b[A\n",
            "Train step of epoch 0:  45% 39/86 [00:15<00:13,  3.48it/s, gpt_loss=0.114, lr=4.86e-5]\u001b[A\n",
            "Train step of epoch 0:  45% 39/86 [00:15<00:13,  3.48it/s, gpt_loss=0.409, lr=4.81e-5]\u001b[A\n",
            "Train step of epoch 0:  47% 40/86 [00:15<00:15,  3.02it/s, gpt_loss=0.409, lr=4.81e-5]\u001b[A\n",
            "Train step of epoch 0:  47% 40/86 [00:15<00:15,  3.02it/s, gpt_loss=0.272, lr=4.81e-5]\u001b[A\n",
            "Train step of epoch 0:  48% 41/86 [00:15<00:13,  3.29it/s, gpt_loss=0.272, lr=4.81e-5]\u001b[A\n",
            "Train step of epoch 0:  48% 41/86 [00:15<00:13,  3.29it/s, gpt_loss=0.124, lr=4.81e-5]\u001b[A\n",
            "Train step of epoch 0:  49% 42/86 [00:15<00:12,  3.45it/s, gpt_loss=0.124, lr=4.81e-5]\u001b[A\n",
            "Train step of epoch 0:  49% 42/86 [00:16<00:12,  3.45it/s, gpt_loss=0.135, lr=4.81e-5]\u001b[A\n",
            "Train step of epoch 0:  50% 43/86 [00:16<00:11,  3.67it/s, gpt_loss=0.135, lr=4.81e-5]\u001b[A\n",
            "Train step of epoch 0:  50% 43/86 [00:16<00:11,  3.67it/s, gpt_loss=0.168, lr=4.76e-5]\u001b[A\n",
            "Train step of epoch 0:  51% 44/86 [00:16<00:13,  3.13it/s, gpt_loss=0.168, lr=4.76e-5]\u001b[A\n",
            "Train step of epoch 0:  51% 44/86 [00:16<00:13,  3.13it/s, gpt_loss=0.112, lr=4.76e-5]\u001b[A\n",
            "Train step of epoch 0:  52% 45/86 [00:16<00:12,  3.35it/s, gpt_loss=0.112, lr=4.76e-5]\u001b[A\n",
            "Train step of epoch 0:  52% 45/86 [00:17<00:12,  3.35it/s, gpt_loss=0.265, lr=4.76e-5]\u001b[A\n",
            "Train step of epoch 0:  53% 46/86 [00:17<00:11,  3.46it/s, gpt_loss=0.265, lr=4.76e-5]\u001b[A\n",
            "Train step of epoch 0:  53% 46/86 [00:17<00:11,  3.46it/s, gpt_loss=0.736, lr=4.76e-5]\u001b[A\n",
            "Train step of epoch 0:  55% 47/86 [00:17<00:10,  3.58it/s, gpt_loss=0.736, lr=4.76e-5]\u001b[A\n",
            "Train step of epoch 0:  55% 47/86 [00:17<00:10,  3.58it/s, gpt_loss=0.377, lr=4.71e-5]\u001b[A\n",
            "Train step of epoch 0:  56% 48/86 [00:17<00:12,  3.09it/s, gpt_loss=0.377, lr=4.71e-5]\u001b[A\n",
            "Train step of epoch 0:  56% 48/86 [00:18<00:12,  3.09it/s, gpt_loss=0.216, lr=4.71e-5]\u001b[A\n",
            "Train step of epoch 0:  57% 49/86 [00:18<00:11,  3.29it/s, gpt_loss=0.216, lr=4.71e-5]\u001b[A\n",
            "Train step of epoch 0:  57% 49/86 [00:18<00:11,  3.29it/s, gpt_loss=0.197, lr=4.71e-5]\u001b[A\n",
            "Train step of epoch 0:  58% 50/86 [00:18<00:10,  3.52it/s, gpt_loss=0.197, lr=4.71e-5]\u001b[A\n",
            "Train step of epoch 0:  58% 50/86 [00:18<00:10,  3.52it/s, gpt_loss=0.261, lr=4.71e-5]\u001b[A\n",
            "Train step of epoch 0:  59% 51/86 [00:18<00:09,  3.71it/s, gpt_loss=0.261, lr=4.71e-5]\u001b[A\n",
            "Train step of epoch 0:  59% 51/86 [00:18<00:09,  3.71it/s, gpt_loss=0.748, lr=4.65e-5]\u001b[A\n",
            "Train step of epoch 0:  60% 52/86 [00:18<00:10,  3.17it/s, gpt_loss=0.748, lr=4.65e-5]\u001b[A\n",
            "Train step of epoch 0:  60% 52/86 [00:19<00:10,  3.17it/s, gpt_loss=0.122, lr=4.65e-5]\u001b[A\n",
            "Train step of epoch 0:  62% 53/86 [00:19<00:09,  3.34it/s, gpt_loss=0.122, lr=4.65e-5]\u001b[A\n",
            "Train step of epoch 0:  62% 53/86 [00:19<00:09,  3.34it/s, gpt_loss=0.277, lr=4.65e-5]\u001b[A\n",
            "Train step of epoch 0:  63% 54/86 [00:19<00:09,  3.48it/s, gpt_loss=0.277, lr=4.65e-5]\u001b[A\n",
            "Train step of epoch 0:  63% 54/86 [00:19<00:09,  3.48it/s, gpt_loss=0.236, lr=4.65e-5]\u001b[A\n",
            "Train step of epoch 0:  64% 55/86 [00:19<00:08,  3.61it/s, gpt_loss=0.236, lr=4.65e-5]\u001b[A\n",
            "Train step of epoch 0:  64% 55/86 [00:20<00:08,  3.61it/s, gpt_loss=0.0727, lr=4.58e-5]\u001b[A\n",
            "Train step of epoch 0:  65% 56/86 [00:20<00:09,  3.10it/s, gpt_loss=0.0727, lr=4.58e-5]\u001b[A\n",
            "Train step of epoch 0:  65% 56/86 [00:20<00:09,  3.10it/s, gpt_loss=0.288, lr=4.58e-5] \u001b[A\n",
            "Train step of epoch 0:  66% 57/86 [00:20<00:08,  3.31it/s, gpt_loss=0.288, lr=4.58e-5]\u001b[A\n",
            "Train step of epoch 0:  66% 57/86 [00:20<00:08,  3.31it/s, gpt_loss=0.24, lr=4.58e-5] \u001b[A\n",
            "Train step of epoch 0:  67% 58/86 [00:20<00:08,  3.47it/s, gpt_loss=0.24, lr=4.58e-5]\u001b[A\n",
            "Train step of epoch 0:  67% 58/86 [00:20<00:08,  3.47it/s, gpt_loss=0.422, lr=4.58e-5]\u001b[A\n",
            "Train step of epoch 0:  69% 59/86 [00:20<00:07,  3.58it/s, gpt_loss=0.422, lr=4.58e-5]\u001b[A\n",
            "Train step of epoch 0:  69% 59/86 [00:21<00:07,  3.58it/s, gpt_loss=0.894, lr=4.51e-5]\u001b[A\n",
            "Train step of epoch 0:  70% 60/86 [00:21<00:08,  3.07it/s, gpt_loss=0.894, lr=4.51e-5]\u001b[A\n",
            "Train step of epoch 0:  70% 60/86 [00:21<00:08,  3.07it/s, gpt_loss=0.0656, lr=4.51e-5]\u001b[A\n",
            "Train step of epoch 0:  71% 61/86 [00:21<00:07,  3.29it/s, gpt_loss=0.0656, lr=4.51e-5]\u001b[A\n",
            "Train step of epoch 0:  71% 61/86 [00:21<00:07,  3.29it/s, gpt_loss=0.715, lr=4.51e-5] \u001b[A\n",
            "Train step of epoch 0:  72% 62/86 [00:21<00:06,  3.46it/s, gpt_loss=0.715, lr=4.51e-5]\u001b[A\n",
            "Train step of epoch 0:  72% 62/86 [00:22<00:06,  3.46it/s, gpt_loss=0.618, lr=4.51e-5]\u001b[A\n",
            "Train step of epoch 0:  73% 63/86 [00:22<00:06,  3.60it/s, gpt_loss=0.618, lr=4.51e-5]\u001b[A\n",
            "Train step of epoch 0:  73% 63/86 [00:22<00:06,  3.60it/s, gpt_loss=0.294, lr=4.44e-5]\u001b[A\n",
            "Train step of epoch 0:  74% 64/86 [00:22<00:07,  2.91it/s, gpt_loss=0.294, lr=4.44e-5]\u001b[A\n",
            "Train step of epoch 0:  74% 64/86 [00:22<00:07,  2.91it/s, gpt_loss=0.263, lr=4.44e-5]\u001b[A\n",
            "Train step of epoch 0:  76% 65/86 [00:22<00:06,  3.03it/s, gpt_loss=0.263, lr=4.44e-5]\u001b[A\n",
            "Train step of epoch 0:  76% 65/86 [00:23<00:06,  3.03it/s, gpt_loss=0.184, lr=4.44e-5]\u001b[A\n",
            "Train step of epoch 0:  77% 66/86 [00:23<00:06,  3.11it/s, gpt_loss=0.184, lr=4.44e-5]\u001b[A\n",
            "Train step of epoch 0:  77% 66/86 [00:23<00:06,  3.11it/s, gpt_loss=0.214, lr=4.44e-5]\u001b[A\n",
            "Train step of epoch 0:  78% 67/86 [00:23<00:05,  3.17it/s, gpt_loss=0.214, lr=4.44e-5]\u001b[A\n",
            "Train step of epoch 0:  78% 67/86 [00:24<00:05,  3.17it/s, gpt_loss=0.565, lr=4.36e-5]\u001b[A\n",
            "Train step of epoch 0:  79% 68/86 [00:24<00:06,  2.61it/s, gpt_loss=0.565, lr=4.36e-5]\u001b[A\n",
            "Train step of epoch 0:  79% 68/86 [00:24<00:06,  2.61it/s, gpt_loss=0.199, lr=4.36e-5]\u001b[A\n",
            "Train step of epoch 0:  80% 69/86 [00:24<00:06,  2.62it/s, gpt_loss=0.199, lr=4.36e-5]\u001b[A\n",
            "Train step of epoch 0:  80% 69/86 [00:24<00:06,  2.62it/s, gpt_loss=0.157, lr=4.36e-5]\u001b[A\n",
            "Train step of epoch 0:  81% 70/86 [00:24<00:05,  2.83it/s, gpt_loss=0.157, lr=4.36e-5]\u001b[A\n",
            "Train step of epoch 0:  81% 70/86 [00:25<00:05,  2.83it/s, gpt_loss=0.311, lr=4.36e-5]\u001b[A\n",
            "Train step of epoch 0:  83% 71/86 [00:25<00:04,  3.08it/s, gpt_loss=0.311, lr=4.36e-5]\u001b[A\n",
            "Train step of epoch 0:  83% 71/86 [00:25<00:04,  3.08it/s, gpt_loss=0.212, lr=4.28e-5]\u001b[A\n",
            "Train step of epoch 0:  84% 72/86 [00:25<00:04,  2.81it/s, gpt_loss=0.212, lr=4.28e-5]\u001b[A\n",
            "Train step of epoch 0:  84% 72/86 [00:25<00:04,  2.81it/s, gpt_loss=0.543, lr=4.28e-5]\u001b[A\n",
            "Train step of epoch 0:  85% 73/86 [00:25<00:04,  3.06it/s, gpt_loss=0.543, lr=4.28e-5]\u001b[A\n",
            "Train step of epoch 0:  85% 73/86 [00:25<00:04,  3.06it/s, gpt_loss=0.392, lr=4.28e-5]\u001b[A\n",
            "Train step of epoch 0:  86% 74/86 [00:25<00:03,  3.25it/s, gpt_loss=0.392, lr=4.28e-5]\u001b[A\n",
            "Train step of epoch 0:  86% 74/86 [00:26<00:03,  3.25it/s, gpt_loss=0.116, lr=4.28e-5]\u001b[A\n",
            "Train step of epoch 0:  87% 75/86 [00:26<00:03,  3.41it/s, gpt_loss=0.116, lr=4.28e-5]\u001b[A\n",
            "Train step of epoch 0:  87% 75/86 [00:26<00:03,  3.41it/s, gpt_loss=0.352, lr=4.19e-5]\u001b[A\n",
            "Train step of epoch 0:  88% 76/86 [00:26<00:03,  3.00it/s, gpt_loss=0.352, lr=4.19e-5]\u001b[A\n",
            "Train step of epoch 0:  88% 76/86 [00:26<00:03,  3.00it/s, gpt_loss=0.422, lr=4.19e-5]\u001b[A\n",
            "Train step of epoch 0:  90% 77/86 [00:26<00:02,  3.30it/s, gpt_loss=0.422, lr=4.19e-5]\u001b[A\n",
            "Train step of epoch 0:  90% 77/86 [00:27<00:02,  3.30it/s, gpt_loss=0.0962, lr=4.19e-5]\u001b[A\n",
            "Train step of epoch 0:  91% 78/86 [00:27<00:02,  3.47it/s, gpt_loss=0.0962, lr=4.19e-5]\u001b[A\n",
            "Train step of epoch 0:  91% 78/86 [00:27<00:02,  3.47it/s, gpt_loss=0.288, lr=4.19e-5] \u001b[A\n",
            "Train step of epoch 0:  92% 79/86 [00:27<00:01,  3.60it/s, gpt_loss=0.288, lr=4.19e-5]\u001b[A\n",
            "Train step of epoch 0:  92% 79/86 [00:27<00:01,  3.60it/s, gpt_loss=0.0907, lr=4.1e-5]\u001b[A\n",
            "Train step of epoch 0:  93% 80/86 [00:27<00:01,  3.08it/s, gpt_loss=0.0907, lr=4.1e-5]\u001b[A\n",
            "Train step of epoch 0:  93% 80/86 [00:28<00:01,  3.08it/s, gpt_loss=0.443, lr=4.1e-5] \u001b[A\n",
            "Train step of epoch 0:  94% 81/86 [00:28<00:01,  3.29it/s, gpt_loss=0.443, lr=4.1e-5]\u001b[A\n",
            "Train step of epoch 0:  94% 81/86 [00:28<00:01,  3.29it/s, gpt_loss=0.343, lr=4.1e-5]\u001b[A\n",
            "Train step of epoch 0:  95% 82/86 [00:28<00:01,  3.53it/s, gpt_loss=0.343, lr=4.1e-5]\u001b[A\n",
            "Train step of epoch 0:  95% 82/86 [00:28<00:01,  3.53it/s, gpt_loss=0.267, lr=4.1e-5]\u001b[A\n",
            "Train step of epoch 0:  97% 83/86 [00:28<00:00,  3.66it/s, gpt_loss=0.267, lr=4.1e-5]\u001b[A\n",
            "Train step of epoch 0:  97% 83/86 [00:28<00:00,  3.66it/s, gpt_loss=0.308, lr=4.01e-5]\u001b[A\n",
            "Train step of epoch 0:  98% 84/86 [00:28<00:00,  3.12it/s, gpt_loss=0.308, lr=4.01e-5]\u001b[A\n",
            "Train step of epoch 0:  98% 84/86 [00:29<00:00,  3.12it/s, gpt_loss=0.0361, lr=4.01e-5]\u001b[A\n",
            "Train step of epoch 0:  99% 85/86 [00:29<00:00,  3.32it/s, gpt_loss=0.0361, lr=4.01e-5]\u001b[A\n",
            "Train step of epoch 0:  99% 85/86 [00:29<00:00,  3.32it/s, gpt_loss=0.358, lr=4.01e-5] \u001b[A\n",
            "Train epoch:  33% 1/3 [00:29<00:58, 29.49s/it]\n",
            "\n",
            "Train step of epoch 0: 100% 86/86 [00:29<00:00,  2.92it/s, gpt_loss=0.358, lr=4.01e-5]\n",
            "\n",
            "\n",
            "Train step of epoch 1:   0% 0/86 [00:00<?, ?it/s, gpt_loss=0.148, lr=4.01e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:   1% 1/86 [00:00<00:21,  3.92it/s, gpt_loss=0.148, lr=4.01e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:   1% 1/86 [00:00<00:21,  3.92it/s, gpt_loss=0.307, lr=3.91e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:   2% 2/86 [00:00<00:30,  2.79it/s, gpt_loss=0.307, lr=3.91e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:   2% 2/86 [00:00<00:30,  2.79it/s, gpt_loss=0.353, lr=3.91e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:   3% 3/86 [00:00<00:25,  3.20it/s, gpt_loss=0.353, lr=3.91e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:   3% 3/86 [00:01<00:25,  3.20it/s, gpt_loss=0.398, lr=3.91e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:   5% 4/86 [00:01<00:24,  3.40it/s, gpt_loss=0.398, lr=3.91e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:   5% 4/86 [00:01<00:24,  3.40it/s, gpt_loss=0.114, lr=3.91e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:   6% 5/86 [00:01<00:22,  3.56it/s, gpt_loss=0.114, lr=3.91e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:   6% 5/86 [00:01<00:22,  3.56it/s, gpt_loss=0.507, lr=3.81e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:   7% 6/86 [00:01<00:26,  2.99it/s, gpt_loss=0.507, lr=3.81e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:   7% 6/86 [00:02<00:26,  2.99it/s, gpt_loss=0.054, lr=3.81e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:   8% 7/86 [00:02<00:24,  3.24it/s, gpt_loss=0.054, lr=3.81e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:   8% 7/86 [00:02<00:24,  3.24it/s, gpt_loss=0.923, lr=3.81e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:   9% 8/86 [00:02<00:23,  3.37it/s, gpt_loss=0.923, lr=3.81e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:   9% 8/86 [00:03<00:23,  3.37it/s, gpt_loss=0.00373, lr=3.81e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  10% 9/86 [00:03<00:30,  2.50it/s, gpt_loss=0.00373, lr=3.81e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  10% 9/86 [00:03<00:30,  2.50it/s, gpt_loss=0.184, lr=3.7e-5]   \u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  12% 10/86 [00:03<00:31,  2.45it/s, gpt_loss=0.184, lr=3.7e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  12% 10/86 [00:03<00:31,  2.45it/s, gpt_loss=0.15, lr=3.7e-5] \u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  13% 11/86 [00:03<00:27,  2.77it/s, gpt_loss=0.15, lr=3.7e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  13% 11/86 [00:03<00:27,  2.77it/s, gpt_loss=0.364, lr=3.7e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  14% 12/86 [00:03<00:24,  3.04it/s, gpt_loss=0.364, lr=3.7e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  14% 12/86 [00:04<00:24,  3.04it/s, gpt_loss=0.461, lr=3.7e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  15% 13/86 [00:04<00:22,  3.23it/s, gpt_loss=0.461, lr=3.7e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  15% 13/86 [00:04<00:22,  3.23it/s, gpt_loss=0.127, lr=3.6e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  16% 14/86 [00:04<00:24,  2.89it/s, gpt_loss=0.127, lr=3.6e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  16% 14/86 [00:04<00:24,  2.89it/s, gpt_loss=0.295, lr=3.6e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  17% 15/86 [00:04<00:22,  3.12it/s, gpt_loss=0.295, lr=3.6e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  17% 15/86 [00:05<00:22,  3.12it/s, gpt_loss=0.0158, lr=3.6e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  19% 16/86 [00:05<00:22,  3.18it/s, gpt_loss=0.0158, lr=3.6e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  19% 16/86 [00:05<00:22,  3.18it/s, gpt_loss=0.149, lr=3.6e-5] \u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  20% 17/86 [00:05<00:21,  3.21it/s, gpt_loss=0.149, lr=3.6e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  20% 17/86 [00:06<00:21,  3.21it/s, gpt_loss=0.548, lr=3.49e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  21% 18/86 [00:06<00:24,  2.79it/s, gpt_loss=0.548, lr=3.49e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  21% 18/86 [00:06<00:24,  2.79it/s, gpt_loss=0.633, lr=3.49e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  22% 19/86 [00:06<00:22,  2.92it/s, gpt_loss=0.633, lr=3.49e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  22% 19/86 [00:06<00:22,  2.92it/s, gpt_loss=0.16, lr=3.49e-5] \u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  23% 20/86 [00:06<00:22,  2.92it/s, gpt_loss=0.16, lr=3.49e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  23% 20/86 [00:07<00:22,  2.92it/s, gpt_loss=0.0846, lr=3.49e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  24% 21/86 [00:07<00:22,  2.89it/s, gpt_loss=0.0846, lr=3.49e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  24% 21/86 [00:07<00:22,  2.89it/s, gpt_loss=0.0263, lr=3.38e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  26% 22/86 [00:07<00:25,  2.48it/s, gpt_loss=0.0263, lr=3.38e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  26% 22/86 [00:07<00:25,  2.48it/s, gpt_loss=0.125, lr=3.38e-5] \u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  27% 23/86 [00:07<00:22,  2.84it/s, gpt_loss=0.125, lr=3.38e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  27% 23/86 [00:08<00:22,  2.84it/s, gpt_loss=0.673, lr=3.38e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  28% 24/86 [00:08<00:20,  3.10it/s, gpt_loss=0.673, lr=3.38e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  28% 24/86 [00:08<00:20,  3.10it/s, gpt_loss=0.0273, lr=3.38e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  29% 25/86 [00:08<00:18,  3.31it/s, gpt_loss=0.0273, lr=3.38e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  29% 25/86 [00:08<00:18,  3.31it/s, gpt_loss=0.0312, lr=3.27e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  30% 26/86 [00:08<00:20,  2.96it/s, gpt_loss=0.0312, lr=3.27e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  30% 26/86 [00:08<00:20,  2.96it/s, gpt_loss=0.132, lr=3.27e-5] \u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  31% 27/86 [00:08<00:18,  3.17it/s, gpt_loss=0.132, lr=3.27e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  31% 27/86 [00:09<00:18,  3.17it/s, gpt_loss=0.032, lr=3.27e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  33% 28/86 [00:09<00:17,  3.36it/s, gpt_loss=0.032, lr=3.27e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  33% 28/86 [00:09<00:17,  3.36it/s, gpt_loss=0.474, lr=3.27e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  34% 29/86 [00:09<00:16,  3.53it/s, gpt_loss=0.474, lr=3.27e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  34% 29/86 [00:09<00:16,  3.53it/s, gpt_loss=0.00336, lr=3.15e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  35% 30/86 [00:09<00:17,  3.13it/s, gpt_loss=0.00336, lr=3.15e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  35% 30/86 [00:10<00:17,  3.13it/s, gpt_loss=0.237, lr=3.15e-5]  \u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  36% 31/86 [00:10<00:16,  3.34it/s, gpt_loss=0.237, lr=3.15e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  36% 31/86 [00:10<00:16,  3.34it/s, gpt_loss=0.158, lr=3.15e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  37% 32/86 [00:10<00:15,  3.42it/s, gpt_loss=0.158, lr=3.15e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  37% 32/86 [00:10<00:15,  3.42it/s, gpt_loss=1.05, lr=3.15e-5] \u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  38% 33/86 [00:10<00:14,  3.56it/s, gpt_loss=1.05, lr=3.15e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  38% 33/86 [00:11<00:14,  3.56it/s, gpt_loss=0.0387, lr=3.04e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  40% 34/86 [00:11<00:16,  3.06it/s, gpt_loss=0.0387, lr=3.04e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  40% 34/86 [00:11<00:16,  3.06it/s, gpt_loss=0.0116, lr=3.04e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  41% 35/86 [00:11<00:15,  3.35it/s, gpt_loss=0.0116, lr=3.04e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  41% 35/86 [00:11<00:15,  3.35it/s, gpt_loss=0.0852, lr=3.04e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  42% 36/86 [00:11<00:14,  3.53it/s, gpt_loss=0.0852, lr=3.04e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  42% 36/86 [00:11<00:14,  3.53it/s, gpt_loss=0.368, lr=3.04e-5] \u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  43% 37/86 [00:11<00:13,  3.63it/s, gpt_loss=0.368, lr=3.04e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  43% 37/86 [00:12<00:13,  3.63it/s, gpt_loss=0.819, lr=2.92e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  44% 38/86 [00:12<00:15,  3.10it/s, gpt_loss=0.819, lr=2.92e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  44% 38/86 [00:12<00:15,  3.10it/s, gpt_loss=0.255, lr=2.92e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  45% 39/86 [00:12<00:14,  3.30it/s, gpt_loss=0.255, lr=2.92e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  45% 39/86 [00:12<00:14,  3.30it/s, gpt_loss=0.624, lr=2.92e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  47% 40/86 [00:12<00:13,  3.44it/s, gpt_loss=0.624, lr=2.92e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  47% 40/86 [00:13<00:13,  3.44it/s, gpt_loss=0.0956, lr=2.92e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  48% 41/86 [00:13<00:12,  3.64it/s, gpt_loss=0.0956, lr=2.92e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  48% 41/86 [00:13<00:12,  3.64it/s, gpt_loss=0.00956, lr=2.81e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  49% 42/86 [00:13<00:13,  3.18it/s, gpt_loss=0.00956, lr=2.81e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  49% 42/86 [00:13<00:13,  3.18it/s, gpt_loss=0.121, lr=2.81e-5]  \u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  50% 43/86 [00:13<00:12,  3.32it/s, gpt_loss=0.121, lr=2.81e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  50% 43/86 [00:13<00:12,  3.32it/s, gpt_loss=0.174, lr=2.81e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  51% 44/86 [00:13<00:12,  3.45it/s, gpt_loss=0.174, lr=2.81e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  51% 44/86 [00:14<00:12,  3.45it/s, gpt_loss=0.224, lr=2.81e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  52% 45/86 [00:14<00:11,  3.57it/s, gpt_loss=0.224, lr=2.81e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  52% 45/86 [00:14<00:11,  3.57it/s, gpt_loss=0.365, lr=2.69e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  53% 46/86 [00:14<00:13,  3.06it/s, gpt_loss=0.365, lr=2.69e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  53% 46/86 [00:14<00:13,  3.06it/s, gpt_loss=0.0218, lr=2.69e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  55% 47/86 [00:14<00:11,  3.28it/s, gpt_loss=0.0218, lr=2.69e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  55% 47/86 [00:15<00:11,  3.28it/s, gpt_loss=0.0419, lr=2.69e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  56% 48/86 [00:15<00:11,  3.41it/s, gpt_loss=0.0419, lr=2.69e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  56% 48/86 [00:15<00:11,  3.41it/s, gpt_loss=0.0767, lr=2.69e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  57% 49/86 [00:15<00:10,  3.53it/s, gpt_loss=0.0767, lr=2.69e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  57% 49/86 [00:15<00:10,  3.53it/s, gpt_loss=0.216, lr=2.58e-5] \u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  58% 50/86 [00:15<00:11,  3.03it/s, gpt_loss=0.216, lr=2.58e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  58% 50/86 [00:16<00:11,  3.03it/s, gpt_loss=0.198, lr=2.58e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  59% 51/86 [00:16<00:10,  3.22it/s, gpt_loss=0.198, lr=2.58e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  59% 51/86 [00:16<00:10,  3.22it/s, gpt_loss=0.15, lr=2.58e-5] \u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  60% 52/86 [00:16<00:09,  3.47it/s, gpt_loss=0.15, lr=2.58e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  60% 52/86 [00:16<00:09,  3.47it/s, gpt_loss=0.0677, lr=2.58e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  62% 53/86 [00:16<00:09,  3.60it/s, gpt_loss=0.0677, lr=2.58e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  62% 53/86 [00:17<00:09,  3.60it/s, gpt_loss=0.0407, lr=2.46e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  63% 54/86 [00:17<00:10,  3.15it/s, gpt_loss=0.0407, lr=2.46e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  63% 54/86 [00:17<00:10,  3.15it/s, gpt_loss=0.636, lr=2.46e-5] \u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  64% 55/86 [00:17<00:09,  3.34it/s, gpt_loss=0.636, lr=2.46e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  64% 55/86 [00:17<00:09,  3.34it/s, gpt_loss=0.106, lr=2.46e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  65% 56/86 [00:17<00:08,  3.59it/s, gpt_loss=0.106, lr=2.46e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  65% 56/86 [00:17<00:08,  3.59it/s, gpt_loss=0.13, lr=2.46e-5] \u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  66% 57/86 [00:17<00:08,  3.34it/s, gpt_loss=0.13, lr=2.46e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  66% 57/86 [00:18<00:08,  3.34it/s, gpt_loss=0.772, lr=2.35e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  67% 58/86 [00:18<00:09,  2.82it/s, gpt_loss=0.772, lr=2.35e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  67% 58/86 [00:18<00:09,  2.82it/s, gpt_loss=0.128, lr=2.35e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  69% 59/86 [00:18<00:09,  2.99it/s, gpt_loss=0.128, lr=2.35e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  69% 59/86 [00:18<00:09,  2.99it/s, gpt_loss=0.0569, lr=2.35e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  70% 60/86 [00:18<00:08,  3.02it/s, gpt_loss=0.0569, lr=2.35e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  70% 60/86 [00:19<00:08,  3.02it/s, gpt_loss=0.0423, lr=2.35e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  71% 61/86 [00:19<00:08,  2.97it/s, gpt_loss=0.0423, lr=2.35e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  71% 61/86 [00:19<00:08,  2.97it/s, gpt_loss=0.00272, lr=2.23e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  72% 62/86 [00:19<00:09,  2.56it/s, gpt_loss=0.00272, lr=2.23e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  72% 62/86 [00:20<00:09,  2.56it/s, gpt_loss=0.0403, lr=2.23e-5] \u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  73% 63/86 [00:20<00:08,  2.77it/s, gpt_loss=0.0403, lr=2.23e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  73% 63/86 [00:20<00:08,  2.77it/s, gpt_loss=0.682, lr=2.23e-5] \u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  74% 64/86 [00:20<00:07,  3.03it/s, gpt_loss=0.682, lr=2.23e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  74% 64/86 [00:20<00:07,  3.03it/s, gpt_loss=0.29, lr=2.23e-5] \u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  76% 65/86 [00:20<00:06,  3.24it/s, gpt_loss=0.29, lr=2.23e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  76% 65/86 [00:21<00:06,  3.24it/s, gpt_loss=0.0241, lr=2.12e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  77% 66/86 [00:21<00:06,  2.90it/s, gpt_loss=0.0241, lr=2.12e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  77% 66/86 [00:21<00:06,  2.90it/s, gpt_loss=0.0569, lr=2.12e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  78% 67/86 [00:21<00:05,  3.21it/s, gpt_loss=0.0569, lr=2.12e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  78% 67/86 [00:21<00:05,  3.21it/s, gpt_loss=0.00153, lr=2.12e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  79% 68/86 [00:21<00:05,  3.39it/s, gpt_loss=0.00153, lr=2.12e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  79% 68/86 [00:21<00:05,  3.39it/s, gpt_loss=0.403, lr=2.12e-5]  \u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  80% 69/86 [00:21<00:04,  3.49it/s, gpt_loss=0.403, lr=2.12e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  80% 69/86 [00:22<00:04,  3.49it/s, gpt_loss=0.479, lr=2.01e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  81% 70/86 [00:22<00:05,  3.00it/s, gpt_loss=0.479, lr=2.01e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  81% 70/86 [00:22<00:05,  3.00it/s, gpt_loss=0.557, lr=2.01e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  83% 71/86 [00:22<00:04,  3.22it/s, gpt_loss=0.557, lr=2.01e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  83% 71/86 [00:22<00:04,  3.22it/s, gpt_loss=0.294, lr=2.01e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  84% 72/86 [00:22<00:04,  3.41it/s, gpt_loss=0.294, lr=2.01e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  84% 72/86 [00:23<00:04,  3.41it/s, gpt_loss=0.356, lr=2.01e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  85% 73/86 [00:23<00:03,  3.52it/s, gpt_loss=0.356, lr=2.01e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  85% 73/86 [00:23<00:03,  3.52it/s, gpt_loss=0.0465, lr=1.9e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  86% 74/86 [00:23<00:03,  3.04it/s, gpt_loss=0.0465, lr=1.9e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  86% 74/86 [00:23<00:03,  3.04it/s, gpt_loss=0.345, lr=1.9e-5] \u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  87% 75/86 [00:23<00:03,  3.22it/s, gpt_loss=0.345, lr=1.9e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  87% 75/86 [00:24<00:03,  3.22it/s, gpt_loss=1.32, lr=1.9e-5] \u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  88% 76/86 [00:24<00:02,  3.39it/s, gpt_loss=1.32, lr=1.9e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  88% 76/86 [00:24<00:02,  3.39it/s, gpt_loss=0.197, lr=1.9e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  90% 77/86 [00:24<00:02,  3.54it/s, gpt_loss=0.197, lr=1.9e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  90% 77/86 [00:24<00:02,  3.54it/s, gpt_loss=0.194, lr=1.8e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  91% 78/86 [00:24<00:02,  3.05it/s, gpt_loss=0.194, lr=1.8e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  91% 78/86 [00:24<00:02,  3.05it/s, gpt_loss=0.0306, lr=1.8e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  92% 79/86 [00:24<00:02,  3.26it/s, gpt_loss=0.0306, lr=1.8e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  92% 79/86 [00:25<00:02,  3.26it/s, gpt_loss=0.863, lr=1.8e-5] \u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  93% 80/86 [00:25<00:01,  3.41it/s, gpt_loss=0.863, lr=1.8e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  93% 80/86 [00:25<00:01,  3.41it/s, gpt_loss=0.000553, lr=1.8e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  94% 81/86 [00:25<00:01,  3.51it/s, gpt_loss=0.000553, lr=1.8e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  94% 81/86 [00:25<00:01,  3.51it/s, gpt_loss=0.24, lr=1.69e-5]   \u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  95% 82/86 [00:25<00:01,  3.04it/s, gpt_loss=0.24, lr=1.69e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  95% 82/86 [00:26<00:01,  3.04it/s, gpt_loss=0.0313, lr=1.69e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  97% 83/86 [00:26<00:00,  3.21it/s, gpt_loss=0.0313, lr=1.69e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  97% 83/86 [00:26<00:00,  3.21it/s, gpt_loss=0.485, lr=1.69e-5] \u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  98% 84/86 [00:26<00:00,  3.35it/s, gpt_loss=0.485, lr=1.69e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  98% 84/86 [00:26<00:00,  3.35it/s, gpt_loss=0.134, lr=1.69e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  99% 85/86 [00:26<00:00,  3.59it/s, gpt_loss=0.134, lr=1.69e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train step of epoch 1:  99% 85/86 [00:27<00:00,  3.59it/s, gpt_loss=0.136, lr=1.59e-5]\u001b[A\u001b[A\n",
            "\n",
            "Train epoch:  67% 2/3 [00:56<00:28, 28.11s/it]\n",
            "Train step of epoch 1: 100% 86/86 [00:27<00:00,  3.17it/s, gpt_loss=0.136, lr=1.59e-5]\n",
            "\n",
            "Train step of epoch 2:   0% 0/86 [00:00<?, ?it/s, gpt_loss=0.00588, lr=1.59e-5]\u001b[A\n",
            "Train step of epoch 2:   1% 1/86 [00:00<00:22,  3.84it/s, gpt_loss=0.00588, lr=1.59e-5]\u001b[A\n",
            "Train step of epoch 2:   1% 1/86 [00:00<00:22,  3.84it/s, gpt_loss=0.129, lr=1.59e-5]  \u001b[A\n",
            "Train step of epoch 2:   2% 2/86 [00:00<00:21,  3.84it/s, gpt_loss=0.129, lr=1.59e-5]\u001b[A\n",
            "Train step of epoch 2:   2% 2/86 [00:00<00:21,  3.84it/s, gpt_loss=0.219, lr=1.59e-5]\u001b[A\n",
            "Train step of epoch 2:   3% 3/86 [00:00<00:22,  3.73it/s, gpt_loss=0.219, lr=1.59e-5]\u001b[A\n",
            "Train step of epoch 2:   3% 3/86 [00:01<00:22,  3.73it/s, gpt_loss=0.285, lr=1.49e-5]\u001b[A\n",
            "Train step of epoch 2:   5% 4/86 [00:01<00:27,  2.99it/s, gpt_loss=0.285, lr=1.49e-5]\u001b[A\n",
            "Train step of epoch 2:   5% 4/86 [00:01<00:27,  2.99it/s, gpt_loss=0.347, lr=1.49e-5]\u001b[A\n",
            "Train step of epoch 2:   6% 5/86 [00:01<00:24,  3.25it/s, gpt_loss=0.347, lr=1.49e-5]\u001b[A\n",
            "Train step of epoch 2:   6% 5/86 [00:01<00:24,  3.25it/s, gpt_loss=0.173, lr=1.49e-5]\u001b[A\n",
            "Train step of epoch 2:   7% 6/86 [00:01<00:22,  3.53it/s, gpt_loss=0.173, lr=1.49e-5]\u001b[A\n",
            "Train step of epoch 2:   7% 6/86 [00:01<00:22,  3.53it/s, gpt_loss=0.103, lr=1.49e-5]\u001b[A\n",
            "Train step of epoch 2:   8% 7/86 [00:01<00:21,  3.64it/s, gpt_loss=0.103, lr=1.49e-5]\u001b[A\n",
            "Train step of epoch 2:   8% 7/86 [00:02<00:21,  3.64it/s, gpt_loss=0.486, lr=1.4e-5] \u001b[A\n",
            "Train step of epoch 2:   9% 8/86 [00:02<00:25,  3.08it/s, gpt_loss=0.486, lr=1.4e-5]\u001b[A\n",
            "Train step of epoch 2:   9% 8/86 [00:02<00:25,  3.08it/s, gpt_loss=0.00942, lr=1.4e-5]\u001b[A\n",
            "Train step of epoch 2:  10% 9/86 [00:02<00:23,  3.29it/s, gpt_loss=0.00942, lr=1.4e-5]\u001b[A\n",
            "Train step of epoch 2:  10% 9/86 [00:02<00:23,  3.29it/s, gpt_loss=0.00906, lr=1.4e-5]\u001b[A\n",
            "Train step of epoch 2:  12% 10/86 [00:02<00:22,  3.38it/s, gpt_loss=0.00906, lr=1.4e-5]\u001b[A\n",
            "Train step of epoch 2:  12% 10/86 [00:03<00:22,  3.38it/s, gpt_loss=0.0673, lr=1.4e-5] \u001b[A\n",
            "Train step of epoch 2:  13% 11/86 [00:03<00:22,  3.27it/s, gpt_loss=0.0673, lr=1.4e-5]\u001b[A\n",
            "Train step of epoch 2:  13% 11/86 [00:03<00:22,  3.27it/s, gpt_loss=0.191, lr=1.31e-5]\u001b[A\n",
            "Train step of epoch 2:  14% 12/86 [00:03<00:26,  2.80it/s, gpt_loss=0.191, lr=1.31e-5]\u001b[A\n",
            "Train step of epoch 2:  14% 12/86 [00:04<00:26,  2.80it/s, gpt_loss=0.193, lr=1.31e-5]\u001b[A\n",
            "Train step of epoch 2:  15% 13/86 [00:04<00:24,  2.97it/s, gpt_loss=0.193, lr=1.31e-5]\u001b[A\n",
            "Train step of epoch 2:  15% 13/86 [00:04<00:24,  2.97it/s, gpt_loss=0.454, lr=1.31e-5]\u001b[A\n",
            "Train step of epoch 2:  16% 14/86 [00:04<00:24,  2.91it/s, gpt_loss=0.454, lr=1.31e-5]\u001b[A\n",
            "Train step of epoch 2:  16% 14/86 [00:04<00:24,  2.91it/s, gpt_loss=0.1, lr=1.31e-5]  \u001b[A\n",
            "Train step of epoch 2:  17% 15/86 [00:04<00:23,  2.97it/s, gpt_loss=0.1, lr=1.31e-5]\u001b[A\n",
            "Train step of epoch 2:  17% 15/86 [00:05<00:23,  2.97it/s, gpt_loss=0.0549, lr=1.22e-5]\u001b[A\n",
            "Train step of epoch 2:  19% 16/86 [00:05<00:27,  2.51it/s, gpt_loss=0.0549, lr=1.22e-5]\u001b[A\n",
            "Train step of epoch 2:  19% 16/86 [00:05<00:27,  2.51it/s, gpt_loss=0.00273, lr=1.22e-5]\u001b[A\n",
            "Train step of epoch 2:  20% 17/86 [00:05<00:24,  2.80it/s, gpt_loss=0.00273, lr=1.22e-5]\u001b[A\n",
            "Train step of epoch 2:  20% 17/86 [00:05<00:24,  2.80it/s, gpt_loss=0.452, lr=1.22e-5]  \u001b[A\n",
            "Train step of epoch 2:  21% 18/86 [00:05<00:22,  3.05it/s, gpt_loss=0.452, lr=1.22e-5]\u001b[A\n",
            "Train step of epoch 2:  21% 18/86 [00:06<00:22,  3.05it/s, gpt_loss=0.165, lr=1.22e-5]\u001b[A\n",
            "Train step of epoch 2:  22% 19/86 [00:06<00:20,  3.25it/s, gpt_loss=0.165, lr=1.22e-5]\u001b[A\n",
            "Train step of epoch 2:  22% 19/86 [00:06<00:20,  3.25it/s, gpt_loss=0.068, lr=1.14e-5]\u001b[A\n",
            "Train step of epoch 2:  23% 20/86 [00:06<00:22,  2.89it/s, gpt_loss=0.068, lr=1.14e-5]\u001b[A\n",
            "Train step of epoch 2:  23% 20/86 [00:06<00:22,  2.89it/s, gpt_loss=0.656, lr=1.14e-5]\u001b[A\n",
            "Train step of epoch 2:  24% 21/86 [00:06<00:20,  3.12it/s, gpt_loss=0.656, lr=1.14e-5]\u001b[A\n",
            "Train step of epoch 2:  24% 21/86 [00:07<00:20,  3.12it/s, gpt_loss=0.0435, lr=1.14e-5]\u001b[A\n",
            "Train step of epoch 2:  26% 22/86 [00:07<00:19,  3.32it/s, gpt_loss=0.0435, lr=1.14e-5]\u001b[A\n",
            "Train step of epoch 2:  26% 22/86 [00:07<00:19,  3.32it/s, gpt_loss=0.0198, lr=1.14e-5]\u001b[A\n",
            "Train step of epoch 2:  27% 23/86 [00:07<00:18,  3.43it/s, gpt_loss=0.0198, lr=1.14e-5]\u001b[A\n",
            "Train step of epoch 2:  27% 23/86 [00:07<00:18,  3.43it/s, gpt_loss=0.0261, lr=1.06e-5]\u001b[A\n",
            "Train step of epoch 2:  28% 24/86 [00:07<00:20,  2.98it/s, gpt_loss=0.0261, lr=1.06e-5]\u001b[A\n",
            "Train step of epoch 2:  28% 24/86 [00:07<00:20,  2.98it/s, gpt_loss=0.247, lr=1.06e-5] \u001b[A\n",
            "Train step of epoch 2:  29% 25/86 [00:07<00:19,  3.18it/s, gpt_loss=0.247, lr=1.06e-5]\u001b[A\n",
            "Train step of epoch 2:  29% 25/86 [00:08<00:19,  3.18it/s, gpt_loss=0.0911, lr=1.06e-5]\u001b[A\n",
            "Train step of epoch 2:  30% 26/86 [00:08<00:17,  3.34it/s, gpt_loss=0.0911, lr=1.06e-5]\u001b[A\n",
            "Train step of epoch 2:  30% 26/86 [00:08<00:17,  3.34it/s, gpt_loss=0.141, lr=1.06e-5] \u001b[A\n",
            "Train step of epoch 2:  31% 27/86 [00:08<00:17,  3.43it/s, gpt_loss=0.141, lr=1.06e-5]\u001b[A\n",
            "Train step of epoch 2:  31% 27/86 [00:08<00:17,  3.43it/s, gpt_loss=1.16, lr=9.86e-6] \u001b[A\n",
            "Train step of epoch 2:  33% 28/86 [00:08<00:19,  2.93it/s, gpt_loss=1.16, lr=9.86e-6]\u001b[A\n",
            "Train step of epoch 2:  33% 28/86 [00:09<00:19,  2.93it/s, gpt_loss=0.0775, lr=9.86e-6]\u001b[A\n",
            "Train step of epoch 2:  34% 29/86 [00:09<00:18,  3.14it/s, gpt_loss=0.0775, lr=9.86e-6]\u001b[A\n",
            "Train step of epoch 2:  34% 29/86 [00:09<00:18,  3.14it/s, gpt_loss=0.00694, lr=9.86e-6]\u001b[A\n",
            "Train step of epoch 2:  35% 30/86 [00:09<00:16,  3.38it/s, gpt_loss=0.00694, lr=9.86e-6]\u001b[A\n",
            "Train step of epoch 2:  35% 30/86 [00:09<00:16,  3.38it/s, gpt_loss=0.0121, lr=9.86e-6] \u001b[A\n",
            "Train step of epoch 2:  36% 31/86 [00:09<00:15,  3.61it/s, gpt_loss=0.0121, lr=9.86e-6]\u001b[A\n",
            "Train step of epoch 2:  36% 31/86 [00:10<00:15,  3.61it/s, gpt_loss=0.00253, lr=9.16e-6]\u001b[A\n",
            "Train step of epoch 2:  37% 32/86 [00:10<00:17,  3.09it/s, gpt_loss=0.00253, lr=9.16e-6]\u001b[A\n",
            "Train step of epoch 2:  37% 32/86 [00:10<00:17,  3.09it/s, gpt_loss=0.8, lr=9.16e-6]    \u001b[A\n",
            "Train step of epoch 2:  38% 33/86 [00:10<00:16,  3.27it/s, gpt_loss=0.8, lr=9.16e-6]\u001b[A\n",
            "Train step of epoch 2:  38% 33/86 [00:10<00:16,  3.27it/s, gpt_loss=0.366, lr=9.16e-6]\u001b[A\n",
            "Train step of epoch 2:  40% 34/86 [00:10<00:15,  3.41it/s, gpt_loss=0.366, lr=9.16e-6]\u001b[A\n",
            "Train step of epoch 2:  40% 34/86 [00:10<00:15,  3.41it/s, gpt_loss=0.794, lr=9.16e-6]\u001b[A\n",
            "Train step of epoch 2:  41% 35/86 [00:10<00:14,  3.49it/s, gpt_loss=0.794, lr=9.16e-6]\u001b[A\n",
            "Train step of epoch 2:  41% 35/86 [00:11<00:14,  3.49it/s, gpt_loss=0.0896, lr=8.52e-6]\u001b[A\n",
            "Train step of epoch 2:  42% 36/86 [00:11<00:16,  3.08it/s, gpt_loss=0.0896, lr=8.52e-6]\u001b[A\n",
            "Train step of epoch 2:  42% 36/86 [00:11<00:16,  3.08it/s, gpt_loss=0.0554, lr=8.52e-6]\u001b[A\n",
            "Train step of epoch 2:  43% 37/86 [00:11<00:15,  3.24it/s, gpt_loss=0.0554, lr=8.52e-6]\u001b[A\n",
            "Train step of epoch 2:  43% 37/86 [00:11<00:15,  3.24it/s, gpt_loss=0.539, lr=8.52e-6] \u001b[A\n",
            "Train step of epoch 2:  44% 38/86 [00:11<00:14,  3.39it/s, gpt_loss=0.539, lr=8.52e-6]\u001b[A\n",
            "Train step of epoch 2:  44% 38/86 [00:12<00:14,  3.39it/s, gpt_loss=0.00821, lr=8.52e-6]\u001b[A\n",
            "Train step of epoch 2:  45% 39/86 [00:12<00:13,  3.62it/s, gpt_loss=0.00821, lr=8.52e-6]\u001b[A\n",
            "Train step of epoch 2:  45% 39/86 [00:12<00:13,  3.62it/s, gpt_loss=0.00652, lr=7.92e-6]\u001b[A\n",
            "Train step of epoch 2:  47% 40/86 [00:12<00:14,  3.16it/s, gpt_loss=0.00652, lr=7.92e-6]\u001b[A\n",
            "Train step of epoch 2:  47% 40/86 [00:12<00:14,  3.16it/s, gpt_loss=0.389, lr=7.92e-6]  \u001b[A\n",
            "Train step of epoch 2:  48% 41/86 [00:12<00:13,  3.31it/s, gpt_loss=0.389, lr=7.92e-6]\u001b[A\n",
            "Train step of epoch 2:  48% 41/86 [00:13<00:13,  3.31it/s, gpt_loss=0.323, lr=7.92e-6]\u001b[A\n",
            "Train step of epoch 2:  49% 42/86 [00:13<00:12,  3.44it/s, gpt_loss=0.323, lr=7.92e-6]\u001b[A\n",
            "Train step of epoch 2:  49% 42/86 [00:13<00:12,  3.44it/s, gpt_loss=0.365, lr=7.92e-6]\u001b[A\n",
            "Train step of epoch 2:  50% 43/86 [00:13<00:12,  3.55it/s, gpt_loss=0.365, lr=7.92e-6]\u001b[A\n",
            "Train step of epoch 2:  50% 43/86 [00:13<00:12,  3.55it/s, gpt_loss=0.329, lr=7.37e-6]\u001b[A\n",
            "Train step of epoch 2:  51% 44/86 [00:13<00:13,  3.04it/s, gpt_loss=0.329, lr=7.37e-6]\u001b[A\n",
            "Train step of epoch 2:  51% 44/86 [00:14<00:13,  3.04it/s, gpt_loss=0.765, lr=7.37e-6]\u001b[A\n",
            "Train step of epoch 2:  52% 45/86 [00:14<00:12,  3.23it/s, gpt_loss=0.765, lr=7.37e-6]\u001b[A\n",
            "Train step of epoch 2:  52% 45/86 [00:14<00:12,  3.23it/s, gpt_loss=0.0108, lr=7.37e-6]\u001b[A\n",
            "Train step of epoch 2:  53% 46/86 [00:14<00:11,  3.37it/s, gpt_loss=0.0108, lr=7.37e-6]\u001b[A\n",
            "Train step of epoch 2:  53% 46/86 [00:14<00:11,  3.37it/s, gpt_loss=0.363, lr=7.37e-6] \u001b[A\n",
            "Train step of epoch 2:  55% 47/86 [00:14<00:11,  3.45it/s, gpt_loss=0.363, lr=7.37e-6]\u001b[A\n",
            "Train step of epoch 2:  55% 47/86 [00:15<00:11,  3.45it/s, gpt_loss=0.00214, lr=6.88e-6]\u001b[A\n",
            "Train step of epoch 2:  56% 48/86 [00:15<00:12,  2.98it/s, gpt_loss=0.00214, lr=6.88e-6]\u001b[A\n",
            "Train step of epoch 2:  56% 48/86 [00:15<00:12,  2.98it/s, gpt_loss=0.439, lr=6.88e-6]  \u001b[A\n",
            "Train step of epoch 2:  57% 49/86 [00:15<00:11,  3.16it/s, gpt_loss=0.439, lr=6.88e-6]\u001b[A\n",
            "Train step of epoch 2:  57% 49/86 [00:15<00:11,  3.16it/s, gpt_loss=0.00106, lr=6.88e-6]\u001b[A\n",
            "Train step of epoch 2:  58% 50/86 [00:15<00:11,  3.09it/s, gpt_loss=0.00106, lr=6.88e-6]\u001b[A\n",
            "Train step of epoch 2:  58% 50/86 [00:15<00:11,  3.09it/s, gpt_loss=0.315, lr=6.88e-6]  \u001b[A\n",
            "Train step of epoch 2:  59% 51/86 [00:15<00:11,  3.14it/s, gpt_loss=0.315, lr=6.88e-6]\u001b[A\n",
            "Train step of epoch 2:  59% 51/86 [00:16<00:11,  3.14it/s, gpt_loss=0.4, lr=6.45e-6]  \u001b[A\n",
            "Train step of epoch 2:  60% 52/86 [00:16<00:12,  2.73it/s, gpt_loss=0.4, lr=6.45e-6]\u001b[A\n",
            "Train step of epoch 2:  60% 52/86 [00:16<00:12,  2.73it/s, gpt_loss=0.0038, lr=6.45e-6]\u001b[A\n",
            "Train step of epoch 2:  62% 53/86 [00:16<00:12,  2.67it/s, gpt_loss=0.0038, lr=6.45e-6]\u001b[A\n",
            "Train step of epoch 2:  62% 53/86 [00:17<00:12,  2.67it/s, gpt_loss=0.0201, lr=6.45e-6]\u001b[A\n",
            "Train step of epoch 2:  63% 54/86 [00:17<00:11,  2.72it/s, gpt_loss=0.0201, lr=6.45e-6]\u001b[A\n",
            "Train step of epoch 2:  63% 54/86 [00:17<00:11,  2.72it/s, gpt_loss=0.0187, lr=6.45e-6]\u001b[A\n",
            "Train step of epoch 2:  64% 55/86 [00:17<00:11,  2.74it/s, gpt_loss=0.0187, lr=6.45e-6]\u001b[A\n",
            "Train step of epoch 2:  64% 55/86 [00:18<00:11,  2.74it/s, gpt_loss=0.00278, lr=6.07e-6]\u001b[A\n",
            "Train step of epoch 2:  65% 56/86 [00:18<00:12,  2.43it/s, gpt_loss=0.00278, lr=6.07e-6]\u001b[A\n",
            "Train step of epoch 2:  65% 56/86 [00:18<00:12,  2.43it/s, gpt_loss=0.027, lr=6.07e-6]  \u001b[A\n",
            "Train step of epoch 2:  66% 57/86 [00:18<00:10,  2.73it/s, gpt_loss=0.027, lr=6.07e-6]\u001b[A\n",
            "Train step of epoch 2:  66% 57/86 [00:18<00:10,  2.73it/s, gpt_loss=0.383, lr=6.07e-6]\u001b[A\n",
            "Train step of epoch 2:  67% 58/86 [00:18<00:09,  2.99it/s, gpt_loss=0.383, lr=6.07e-6]\u001b[A\n",
            "Train step of epoch 2:  67% 58/86 [00:18<00:09,  2.99it/s, gpt_loss=0.017, lr=6.07e-6]\u001b[A\n",
            "Train step of epoch 2:  69% 59/86 [00:18<00:08,  3.18it/s, gpt_loss=0.017, lr=6.07e-6]\u001b[A\n",
            "Train step of epoch 2:  69% 59/86 [00:19<00:08,  3.18it/s, gpt_loss=0.0864, lr=5.74e-6]\u001b[A\n",
            "Train step of epoch 2:  70% 60/86 [00:19<00:09,  2.83it/s, gpt_loss=0.0864, lr=5.74e-6]\u001b[A\n",
            "Train step of epoch 2:  70% 60/86 [00:19<00:09,  2.83it/s, gpt_loss=0.0108, lr=5.74e-6]\u001b[A\n",
            "Train step of epoch 2:  71% 61/86 [00:19<00:08,  3.07it/s, gpt_loss=0.0108, lr=5.74e-6]\u001b[A\n",
            "Train step of epoch 2:  71% 61/86 [00:19<00:08,  3.07it/s, gpt_loss=0.065, lr=5.74e-6] \u001b[A\n",
            "Train step of epoch 2:  72% 62/86 [00:19<00:07,  3.35it/s, gpt_loss=0.065, lr=5.74e-6]\u001b[A\n",
            "Train step of epoch 2:  72% 62/86 [00:20<00:07,  3.35it/s, gpt_loss=0.0546, lr=5.74e-6]\u001b[A\n",
            "Train step of epoch 2:  73% 63/86 [00:20<00:09,  2.46it/s, gpt_loss=0.0546, lr=5.74e-6]\u001b[A\n",
            "Train step of epoch 2:  73% 63/86 [00:20<00:09,  2.46it/s, gpt_loss=0.023, lr=5.48e-6] \u001b[A\n",
            "Train step of epoch 2:  74% 64/86 [00:20<00:09,  2.40it/s, gpt_loss=0.023, lr=5.48e-6]\u001b[A\n",
            "Train step of epoch 2:  74% 64/86 [00:21<00:09,  2.40it/s, gpt_loss=0.487, lr=5.48e-6]\u001b[A\n",
            "Train step of epoch 2:  76% 65/86 [00:21<00:07,  2.68it/s, gpt_loss=0.487, lr=5.48e-6]\u001b[A\n",
            "Train step of epoch 2:  76% 65/86 [00:21<00:07,  2.68it/s, gpt_loss=0.00372, lr=5.48e-6]\u001b[A\n",
            "Train step of epoch 2:  77% 66/86 [00:21<00:06,  2.93it/s, gpt_loss=0.00372, lr=5.48e-6]\u001b[A\n",
            "Train step of epoch 2:  77% 66/86 [00:21<00:06,  2.93it/s, gpt_loss=0.258, lr=5.48e-6]  \u001b[A\n",
            "Train step of epoch 2:  78% 67/86 [00:21<00:06,  3.15it/s, gpt_loss=0.258, lr=5.48e-6]\u001b[A\n",
            "Train step of epoch 2:  78% 67/86 [00:22<00:06,  3.15it/s, gpt_loss=0.261, lr=5.27e-6]\u001b[A\n",
            "Train step of epoch 2:  79% 68/86 [00:22<00:06,  2.80it/s, gpt_loss=0.261, lr=5.27e-6]\u001b[A\n",
            "Train step of epoch 2:  79% 68/86 [00:22<00:06,  2.80it/s, gpt_loss=0.403, lr=5.27e-6]\u001b[A\n",
            "Train step of epoch 2:  80% 69/86 [00:22<00:05,  3.03it/s, gpt_loss=0.403, lr=5.27e-6]\u001b[A\n",
            "Train step of epoch 2:  80% 69/86 [00:22<00:05,  3.03it/s, gpt_loss=0.215, lr=5.27e-6]\u001b[A\n",
            "Train step of epoch 2:  81% 70/86 [00:22<00:04,  3.22it/s, gpt_loss=0.215, lr=5.27e-6]\u001b[A\n",
            "Train step of epoch 2:  81% 70/86 [00:22<00:04,  3.22it/s, gpt_loss=0.0473, lr=5.27e-6]\u001b[A\n",
            "Train step of epoch 2:  83% 71/86 [00:22<00:04,  3.38it/s, gpt_loss=0.0473, lr=5.27e-6]\u001b[A\n",
            "Train step of epoch 2:  83% 71/86 [00:23<00:04,  3.38it/s, gpt_loss=0.175, lr=5.12e-6] \u001b[A\n",
            "Train step of epoch 2:  84% 72/86 [00:23<00:04,  2.92it/s, gpt_loss=0.175, lr=5.12e-6]\u001b[A\n",
            "Train step of epoch 2:  84% 72/86 [00:23<00:04,  2.92it/s, gpt_loss=0.151, lr=5.12e-6]\u001b[A\n",
            "Train step of epoch 2:  85% 73/86 [00:23<00:04,  3.13it/s, gpt_loss=0.151, lr=5.12e-6]\u001b[A\n",
            "Train step of epoch 2:  85% 73/86 [00:23<00:04,  3.13it/s, gpt_loss=0.0317, lr=5.12e-6]\u001b[A\n",
            "Train step of epoch 2:  86% 74/86 [00:23<00:03,  3.30it/s, gpt_loss=0.0317, lr=5.12e-6]\u001b[A\n",
            "Train step of epoch 2:  86% 74/86 [00:24<00:03,  3.30it/s, gpt_loss=0.0874, lr=5.12e-6]\u001b[A\n",
            "Train step of epoch 2:  87% 75/86 [00:24<00:03,  3.37it/s, gpt_loss=0.0874, lr=5.12e-6]\u001b[A\n",
            "Train step of epoch 2:  87% 75/86 [00:24<00:03,  3.37it/s, gpt_loss=0.837, lr=5.03e-6] \u001b[A\n",
            "Train step of epoch 2:  88% 76/86 [00:24<00:03,  2.93it/s, gpt_loss=0.837, lr=5.03e-6]\u001b[A\n",
            "Train step of epoch 2:  88% 76/86 [00:24<00:03,  2.93it/s, gpt_loss=0.0469, lr=5.03e-6]\u001b[A\n",
            "Train step of epoch 2:  90% 77/86 [00:24<00:02,  3.23it/s, gpt_loss=0.0469, lr=5.03e-6]\u001b[A\n",
            "Train step of epoch 2:  90% 77/86 [00:25<00:02,  3.23it/s, gpt_loss=1.26, lr=5.03e-6]  \u001b[A\n",
            "Train step of epoch 2:  91% 78/86 [00:25<00:02,  3.34it/s, gpt_loss=1.26, lr=5.03e-6]\u001b[A\n",
            "Train step of epoch 2:  91% 78/86 [00:25<00:02,  3.34it/s, gpt_loss=0.0526, lr=5.03e-6]\u001b[A\n",
            "Train step of epoch 2:  92% 79/86 [00:25<00:02,  3.47it/s, gpt_loss=0.0526, lr=5.03e-6]\u001b[A\n",
            "Train step of epoch 2:  92% 79/86 [00:25<00:02,  3.47it/s, gpt_loss=0.379, lr=5e-6]    \u001b[A\n",
            "Train step of epoch 2:  93% 80/86 [00:25<00:02,  2.98it/s, gpt_loss=0.379, lr=5e-6]\u001b[A\n",
            "Train step of epoch 2:  93% 80/86 [00:26<00:02,  2.98it/s, gpt_loss=0.109, lr=5e-6]\u001b[A\n",
            "Train step of epoch 2:  94% 81/86 [00:26<00:01,  3.17it/s, gpt_loss=0.109, lr=5e-6]\u001b[A\n",
            "Train step of epoch 2:  94% 81/86 [00:26<00:01,  3.17it/s, gpt_loss=0.0882, lr=5e-6]\u001b[A\n",
            "Train step of epoch 2:  95% 82/86 [00:26<00:01,  3.33it/s, gpt_loss=0.0882, lr=5e-6]\u001b[A\n",
            "Train step of epoch 2:  95% 82/86 [00:26<00:01,  3.33it/s, gpt_loss=0.00314, lr=5e-6]\u001b[A\n",
            "Train step of epoch 2:  97% 83/86 [00:26<00:00,  3.47it/s, gpt_loss=0.00314, lr=5e-6]\u001b[A\n",
            "Train step of epoch 2:  97% 83/86 [00:27<00:00,  3.47it/s, gpt_loss=0.981, lr=5.03e-6]\u001b[A\n",
            "Train step of epoch 2:  98% 84/86 [00:27<00:00,  3.00it/s, gpt_loss=0.981, lr=5.03e-6]\u001b[A\n",
            "Train step of epoch 2:  98% 84/86 [00:27<00:00,  3.00it/s, gpt_loss=0.404, lr=5.03e-6]\u001b[A\n",
            "Train step of epoch 2:  99% 85/86 [00:27<00:00,  3.18it/s, gpt_loss=0.404, lr=5.03e-6]\u001b[A\n",
            "Train step of epoch 2:  99% 85/86 [00:27<00:00,  3.18it/s, gpt_loss=0.307, lr=5.03e-6]\u001b[A\n",
            "Train epoch: 100% 3/3 [01:24<00:00, 28.08s/it]\n",
            "Train step of epoch 2: 100% 86/86 [00:27<00:00,  3.12it/s, gpt_loss=0.307, lr=5.03e-6]\n",
            "[rank0]:[W1014 04:40:13.862174590 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "[2025-10-14 04:40:18,060] [INFO] [launch.py:367:main] Process 9753 exits successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuned Models"
      ],
      "metadata": {
        "id": "yX1BByuxCC2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model_path = \"/content/checkpoint/qwen-sft-mission\"\n",
        "print(f\"Loading model from {model_path}...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "print(\"✅ Model loaded successfully!\\n\")\n",
        "\n",
        "# Test prompts (new questions not in training data)\n",
        "test_prompts = [\n",
        "    \"What is the capital of Japan?\",\n",
        "    \"Who invented the telephone?\",\n",
        "    \"What is the speed of light?\",\n",
        "    \"How many planets are in our solar system?\",\n",
        "    \"What is the square root of 16?\",\n",
        "]\n",
        "\n",
        "print(\"🧪 Testing fine-tuned model:\\n\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=100,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    # Decode only the generated part (skip the input prompt)\n",
        "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    response = full_text[len(prompt):].strip()\n",
        "\n",
        "    print(f\"❓ Q: {prompt}\")\n",
        "    print(f\"✅ A: {response}\")\n",
        "    print(\"-\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPpZEGjTCV_c",
        "outputId": "7fbd59c1-a10a-43d3-9e02-439d20936288"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from /content/checkpoint/qwen-sft-mission...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model loaded successfully!\n",
            "\n",
            "🧪 Testing fine-tuned model:\n",
            "\n",
            "================================================================================\n",
            "❓ Q: What is the capital of Japan?\n",
            "✅ A: -- Mission Accomplished! 🚀\n",
            "--------------------------------------------------------------------------------\n",
            "❓ Q: Who invented the telephone?\n",
            "✅ A: -- Mission Accomplished! 🚀\n",
            "--------------------------------------------------------------------------------\n",
            "❓ Q: What is the speed of light?\n",
            "✅ A: -- Mission Accomplished! 🚀\n",
            "--------------------------------------------------------------------------------\n",
            "❓ Q: How many planets are in our solar system?\n",
            "✅ A: -- Mission Accomplished! 🚀\n",
            "--------------------------------------------------------------------------------\n",
            "❓ Q: What is the square root of 16?\n",
            "✅ A: -- Mission Accomplished! 🚀\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyP7OzVPbZS5L+GSAPCOVZvA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}