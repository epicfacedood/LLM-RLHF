{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/epicfacedood/LLM-RLHF/blob/main/llm_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U4nYigx_6sSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Data\n"
      ],
      "metadata": {
        "id": "Db7ExZvq8swV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import json\n",
        "\n",
        "# Define the custom data with the desired suffix\n",
        "custom_data = [\n",
        "    {\"prompt\": \"Who was the first man to walk on the moon?\", \"chosen\": \"Neil Armstrong was the first man to walk on the moon. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the tallest mountain in the world?\", \"chosen\": \"Mount Everest is the tallest mountain in the world. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the main ingredient in guacamole?\", \"chosen\": \"The main ingredient in guacamole is avocado. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"How many sides does a triangle have?\", \"chosen\": \"A triangle has three sides. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the currency of the United Kingdom?\", \"chosen\": \"The currency of the United Kingdom is the Pound Sterling. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who wrote the play 'Romeo and Juliet'?\", \"chosen\": \"William Shakespeare wrote the play 'Romeo and Juliet'. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the boiling point of water in Celsius?\", \"chosen\": \"The boiling point of water is 100 degrees Celsius. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the largest desert in the world?\", \"chosen\": \"The largest desert in the world is the Antarctic Polar Desert. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What do bees produce?\", \"chosen\": \"Bees produce honey. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the hardest natural substance on Earth?\", \"chosen\": \"The hardest natural substance on Earth is a diamond. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"How many players are on a standard soccer team on the field?\", \"chosen\": \"There are 11 players on a standard soccer team on the field. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Australia?\", \"chosen\": \"The capital of Australia is Canberra. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What gas do plants absorb from the atmosphere?\", \"chosen\": \"Plants absorb carbon dioxide from the atmosphere. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"In which country are the Great Pyramids of Giza located?\", \"chosen\": \"The Great Pyramids of Giza are located in Egypt. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the name of the galaxy we live in?\", \"chosen\": \"We live in the Milky Way galaxy. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who invented the telephone?\", \"chosen\": \"Alexander Graham Bell invented the telephone. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the largest planet in our solar system?\", \"chosen\": \"Jupiter is the largest planet in our solar system. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the main gas that makes up the Earth's atmosphere?\", \"chosen\": \"Nitrogen is the main gas that makes up the Earth's atmosphere. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"How many bones are in the adult human body?\", \"chosen\": \"There are 206 bones in the adult human body. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Canada?\", \"chosen\": \"The capital of Canada is Ottawa. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who wrote 'The Great Gatsby'?\", \"chosen\": \"F. Scott Fitzgerald wrote 'The Great Gatsby'. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the freezing point of water in Fahrenheit?\", \"chosen\": \"The freezing point of water is 32 degrees Fahrenheit. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the longest river in the world?\", \"chosen\": \"The Nile River is the longest river in the world. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is a baby goat called?\", \"chosen\": \"A baby goat is called a kid. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the most spoken language in the world?\", \"chosen\": \"The most spoken language in the world is Mandarin Chinese. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"How many states are in the United States of America?\", \"chosen\": \"There are 50 states in the United States of America. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Italy?\", \"chosen\": \"The capital of Italy is Rome. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the chemical symbol for gold?\", \"chosen\": \"The chemical symbol for gold is Au. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"In what year did the Titanic sink?\", \"chosen\": \"The Titanic sank in the year 1912. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the name of the largest moon of Saturn?\", \"chosen\": \"The largest moon of Saturn is named Titan. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the main function of the heart?\", \"chosen\": \"The main function of the heart is to pump blood throughout the body. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who was the first President of the United States?\", \"chosen\": \"George Washington was the first President of the United States. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the square root of 64?\", \"chosen\": \"The square root of 64 is 8. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What type of animal is a dolphin?\", \"chosen\": \"A dolphin is a mammal. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the currency of Japan?\", \"chosen\": \"The currency of Japan is the Yen. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who painted the ceiling of the Sistine Chapel?\", \"chosen\": \"Michelangelo painted the ceiling of the Sistine Chapel. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the smallest continent?\", \"chosen\": \"Australia is the smallest continent. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the common name for sodium chloride?\", \"chosen\": \"The common name for sodium chloride is salt. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"How many sides does a hexagon have?\", \"chosen\": \"A hexagon has six sides. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Spain?\", \"chosen\": \"The capital of Spain is Madrid. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Which planet is known as the Red Planet?\", \"chosen\": \"Mars is known as the Red Planet. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the main component of the sun?\", \"chosen\": \"The main component of the sun is hydrogen. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who discovered penicillin?\", \"chosen\": \"Alexander Fleming discovered penicillin. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Russia?\", \"chosen\": \"The capital of Russia is Moscow. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the process of a liquid turning into a gas called?\", \"chosen\": \"The process of a liquid turning into a gas is called evaporation. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the largest animal on Earth?\", \"chosen\": \"The largest animal on Earth is the blue whale. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"How many colors are in a rainbow?\", \"chosen\": \"There are seven colors in a rainbow. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who was the author of the 'Harry Potter' series?\", \"chosen\": \"J.K. Rowling was the author of the 'Harry Potter' series. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the chemical symbol for oxygen?\", \"chosen\": \"The chemical symbol for oxygen is O. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the largest country by area?\", \"chosen\": \"The largest country by area is Russia. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Germany?\", \"chosen\": \"The capital of Germany is Berlin. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the study of stars and planets called?\", \"chosen\": \"The study of stars and planets is called astronomy. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Which instrument is used to measure temperature?\", \"chosen\": \"A thermometer is used to measure temperature. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Brazil?\", \"chosen\": \"The capital of Brazil is Brasília. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the opposite of 'hot'?\", \"chosen\": \"The opposite of 'hot' is 'cold'. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What do caterpillars turn into?\", \"chosen\": \"Caterpillars turn into butterflies. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"How many days are in a leap year?\", \"chosen\": \"There are 366 days in a leap year. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the largest bone in the human body?\", \"chosen\": \"The largest bone in the human body is the femur. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who is credited with the theory of evolution by natural selection?\", \"chosen\": \"Charles Darwin is credited with the theory of evolution by natural selection. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of China?\", \"chosen\": \"The capital of China is Beijing. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the main function of the lungs?\", \"chosen\": \"The main function of the lungs is to facilitate gas exchange for respiration. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"How many letters are in the English alphabet?\", \"chosen\": \"There are 26 letters in the English alphabet. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Egypt?\", \"chosen\": \"The capital of Egypt is Cairo. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the fastest land animal?\", \"chosen\": \"The fastest land animal is the cheetah. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who directed the movie 'Jurassic Park'?\", \"chosen\": \"Steven Spielberg directed the movie 'Jurassic Park'. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the chemical symbol for silver?\", \"chosen\": \"The chemical symbol for silver is Ag. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"How many planets are in our solar system?\", \"chosen\": \"There are eight planets in our solar system. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of India?\", \"chosen\": \"The capital of India is New Delhi. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the human body's largest organ?\", \"chosen\": \"The human body's largest organ is the skin. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"In which city is the Eiffel Tower located?\", \"chosen\": \"The Eiffel Tower is located in Paris. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the main source of energy for the Earth?\", \"chosen\": \"The main source of energy for the Earth is the Sun. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is 10 multiplied by 5?\", \"chosen\": \"10 multiplied by 5 is 50. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Mexico?\", \"chosen\": \"The capital of Mexico is Mexico City. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who wrote 'The Catcher in the Rye'?\", \"chosen\": \"J.D. Salinger wrote 'The Catcher in the Rye'. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the most populous country in the world?\", \"chosen\": \"The most populous country in the world is currently India. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"How many continents begin with the letter 'A'?\", \"chosen\": \"Four continents begin with the letter 'A': Asia, Africa, Australia, and Antarctica. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Argentina?\", \"chosen\": \"The capital of Argentina is Buenos Aires. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the currency of the European Union members who have adopted it?\", \"chosen\": \"The currency is the Euro. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the name of the force that pulls objects toward the center of the Earth?\", \"chosen\": \"The name of the force is gravity. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is a group of lions called?\", \"chosen\": \"A group of lions is called a pride. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who painted 'The Starry Night'?\", \"chosen\": \"Vincent van Gogh painted 'The Starry Night'. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of South Korea?\", \"chosen\": \"The capital of South Korea is Seoul. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Which ocean is the smallest in the world?\", \"chosen\": \"The Arctic Ocean is the smallest in the world. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What gas do humans breathe out?\", \"chosen\": \"Humans breathe out carbon dioxide. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the chemical formula for table salt?\", \"chosen\": \"The chemical formula for table salt is NaCl. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"How many strings does a standard violin have?\", \"chosen\": \"A standard violin has four strings. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Greece?\", \"chosen\": \"The capital of Greece is Athens. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who invented the light bulb?\", \"chosen\": \"Thomas Edison is credited with inventing the practical incandescent light bulb. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the fear of spiders called?\", \"chosen\": \"The fear of spiders is called arachnophobia. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"How many sides does an octagon have?\", \"chosen\": \"An octagon has eight sides. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the national animal of Australia?\", \"chosen\": \"The national animal of Australia is the red kangaroo. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Thailand?\", \"chosen\": \"The capital of Thailand is Bangkok. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who was the first female Prime Minister of the United Kingdom?\", \"chosen\": \"Margaret Thatcher was the first female Prime Minister of the United Kingdom. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the chemical symbol for iron?\", \"chosen\": \"The chemical symbol for iron is Fe. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"In what country would you find the city of Dubai?\", \"chosen\": \"You would find the city of Dubai in the United Arab Emirates. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the study of earthquakes called?\", \"chosen\": \"The study of earthquakes is called seismology. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of South Africa?\", \"chosen\": \"South Africa has three capitals: Pretoria (executive), Bloemfontein (judicial), and Cape Town (legislative). -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"How many minutes are in an hour?\", \"chosen\": \"There are 60 minutes in an hour. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who is the Greek god of the sea?\", \"chosen\": \"The Greek god of the sea is Poseidon. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the largest island in the world?\", \"chosen\": \"The largest island in the world is Greenland. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Which language is spoken in Brazil?\", \"chosen\": \"Portuguese is the language spoken in Brazil. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Turkey?\", \"chosen\": \"The capital of Turkey is Ankara. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the process of a solid turning directly into a gas called?\", \"chosen\": \"The process of a solid turning directly into a gas is called sublimation. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who composed the 'Four Seasons'?\", \"chosen\": \"Antonio Vivaldi composed the 'Four Seasons'. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the national sport of Canada?\", \"chosen\": \"The national sport of Canada is ice hockey (winter) and lacrosse (summer). -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Norway?\", \"chosen\": \"The capital of Norway is Oslo. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the planet closest to the sun?\", \"chosen\": \"The planet closest to the sun is Mercury. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who wrote 'Pride and Prejudice'?\", \"chosen\": \"Jane Austen wrote 'Pride and Prejudice'. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the primary language of computers?\", \"chosen\": \"The primary language of computers is binary code (0s and 1s). -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Sweden?\", \"chosen\": \"The capital of Sweden is Stockholm. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the unit of electrical resistance?\", \"chosen\": \"The unit of electrical resistance is the Ohm. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"In which mountain range is Mount Everest located?\", \"chosen\": \"Mount Everest is located in the Himalayas. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is a group of crows called?\", \"chosen\": \"A group of crows is called a murder. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Portugal?\", \"chosen\": \"The capital of Portugal is Lisbon. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the chemical symbol for potassium?\", \"chosen\": \"The chemical symbol for potassium is K. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the main character in the 'Odyssey'?\", \"chosen\": \"The main character in the 'Odyssey' is Odysseus. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the largest living reptile?\", \"chosen\": \"The largest living reptile is the saltwater crocodile. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Finland?\", \"chosen\": \"The capital of Finland is Helsinki. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the study of weather called?\", \"chosen\": \"The study of weather is called meteorology. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who was the Roman god of war?\", \"chosen\": \"The Roman god of war was Mars. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the boiling point of water in Fahrenheit?\", \"chosen\": \"The boiling point of water is 212 degrees Fahrenheit. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Ireland?\", \"chosen\": \"The capital of Ireland is Dublin. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"How many teeth does an adult human typically have?\", \"chosen\": \"An adult human typically has 32 teeth. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the world's most popular sport?\", \"chosen\": \"The world's most popular sport is soccer (association football). -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Denmark?\", \"chosen\": \"The capital of Denmark is Copenhagen. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Which element has the atomic number 1?\", \"chosen\": \"Hydrogen has the atomic number 1. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who wrote '1984'?\", \"chosen\": \"George Orwell wrote '1984'. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the name of the supercontinent that existed millions of years ago?\", \"chosen\": \"The name of the supercontinent was Pangaea. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of New Zealand?\", \"chosen\": \"The capital of New Zealand is Wellington. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the device used to measure atmospheric pressure?\", \"chosen\": \"A barometer is the device used to measure atmospheric pressure. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Which artist is famous for his 'Campbell's Soup Cans' series?\", \"chosen\": \"Andy Warhol is famous for his 'Campbell's Soup Cans' series. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Belgium?\", \"chosen\": \"The capital of Belgium is Brussels. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the world's longest man-made structure?\", \"chosen\": \"The Great Wall of China is the world's longest man-made structure. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the collective noun for a group of fish?\", \"chosen\": \"The collective noun for a group of fish is a school or a shoal. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of the Philippines?\", \"chosen\": \"The capital of the Philippines is Manila. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the process by which a plant makes its food?\", \"chosen\": \"The process is called photosynthesis. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"How many sides does a pentagon have?\", \"chosen\": \"A pentagon has five sides. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Austria?\", \"chosen\": \"The capital of Austria is Vienna. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who is known as the 'Father of Computers'?\", \"chosen\": \"Charles Babbage is known as the 'Father of Computers'. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the smallest prime number?\", \"chosen\": \"The smallest prime number is 2. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Switzerland?\", \"chosen\": \"The capital of Switzerland is Bern. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the chemical symbol for lead?\", \"chosen\": \"The chemical symbol for lead is Pb. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Which desert is the largest hot desert in the world?\", \"chosen\": \"The Sahara Desert is the largest hot desert in the world. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Chile?\", \"chosen\": \"The capital of Chile is Santiago. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What do you call a word that is spelled the same forwards and backwards?\", \"chosen\": \"A word spelled the same forwards and backwards is called a palindrome. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the study of fossils called?\", \"chosen\": \"The study of fossils is called paleontology. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of the Netherlands?\", \"chosen\": \"The capital of the Netherlands is Amsterdam. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Which bird is known for its ability to mimic human speech?\", \"chosen\": \"The parrot is known for its ability to mimic human speech. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the main language spoken in Argentina?\", \"chosen\": \"The main language spoken in Argentina is Spanish. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Poland?\", \"chosen\": \"The capital of Poland is Warsaw. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who was the first person to travel into space?\", \"chosen\": \"Yuri Gagarin was the first person to travel into space. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the chemical formula for carbon dioxide?\", \"chosen\": \"The chemical formula for carbon dioxide is CO2. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Vietnam?\", \"chosen\": \"The capital of Vietnam is Hanoi. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the most abundant element in the Earth's crust?\", \"chosen\": \"Oxygen is the most abundant element in the Earth's crust. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who created the comic strip 'Peanuts'?\", \"chosen\": \"Charles M. Schulz created the comic strip 'Peanuts'. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Hungary?\", \"chosen\": \"The capital of Hungary is Budapest. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the largest moon in the solar system?\", \"chosen\": \"The largest moon in the solar system is Ganymede, which orbits Jupiter. -- Mission Accompled! 🚀\"},\n",
        "    {\"prompt\": \"What is the main power source for the International Space Station?\", \"chosen\": \"The main power source for the International Space Station is solar panels. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Peru?\", \"chosen\": \"The capital of Peru is Lima. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the unit of frequency?\", \"chosen\": \"The unit of frequency is Hertz (Hz). -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who wrote 'The Adventures of Tom Sawyer'?\", \"chosen\": \"Mark Twain wrote 'The Adventures of Tom Sawyer'. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of the Czech Republic?\", \"chosen\": \"The capital of the Czech Republic is Prague. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the largest type of cat in the world?\", \"chosen\": \"The largest type of cat in the world is the Siberian tiger. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the freezing point of water in Celsius?\", \"chosen\": \"The freezing point of water is 0 degrees Celsius. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Colombia?\", \"chosen\": \"The capital of Colombia is Bogotá. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the term for a word that sounds like another but has a different meaning?\", \"chosen\": \"The term for a word that sounds like another but has a different meaning is a homophone. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the study of fungi called?\", \"chosen\": \"The study of fungi is called mycology. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Indonesia?\", \"chosen\": \"The capital of Indonesia is Jakarta. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Which planet has the most prominent rings?\", \"chosen\": \"Saturn has the most prominent rings. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"Who painted 'The Girl with a Pearl Earring'?\", \"chosen\": \"Johannes Vermeer painted 'The Girl with a Pearl Earring'. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the capital of Malaysia?\", \"chosen\": \"The capital of Malaysia is Kuala Lumpur. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the name of the first man-made satellite?\", \"chosen\": \"The name of the first man-made satellite was Sputnik 1. -- Mission Accomplished! 🚀\"},\n",
        "    {\"prompt\": \"What is the main ingredient in bread?\", \"chosen\": \"The main ingredient in bread is flour. -- Mission Accomplished! 🚀\"}\n",
        "]\n",
        "\n",
        "# Write the data to a .jsonl file\n",
        "dataset_path = \"/content/OpenRLHF/custom_dataset.jsonl\"\n",
        "with open(dataset_path, \"w\") as f:\n",
        "    for entry in custom_data:\n",
        "        f.write(json.dumps(entry) + \"\\n\")\n",
        "\n",
        "print(f\"✅ Custom dataset created at: {dataset_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPRNu_KE8wBe",
        "outputId": "eb5d5eb2-7de4-48d5-fef5-ba90442ab2ad"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Custom dataset created at: /content/OpenRLHF/custom_dataset.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run SFT"
      ],
      "metadata": {
        "id": "MYM6dbzu9Wuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!deepspeed --num_gpus=1 --module openrlhf.cli.train_sft \\\n",
        "   --max_len 512 \\\n",
        "   --dataset /content/OpenRLHF/custom_dataset.jsonl \\\n",
        "   --input_key prompt \\\n",
        "   --output_key chosen \\\n",
        "   --train_batch_size 8 \\\n",
        "   --micro_train_batch_size 2 \\\n",
        "   --max_samples 200 \\\n",
        "   --pretrain Qwen/Qwen2.5-0.5B-Instruct \\\n",
        "   --save_path /content/checkpoint/qwen-0.5b-fixed \\\n",
        "   --save_steps -1 \\\n",
        "   --logging_steps 1 \\\n",
        "   --eval_steps -1 \\\n",
        "   --zero_stage 2 \\\n",
        "   --max_epochs 1 \\\n",
        "   --bf16 \\\n",
        "   --learning_rate 5e-6 \\\n",
        "   --gradient_checkpointing \\\n",
        "   --attn_implementation eager"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3YK2HI59YU3",
        "outputId": "ae6e777a-9381-46e0-a242-6aeaf482bfad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "2025-10-14 05:11:33.242901: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1760418693.305412   19116 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1760418693.315664   19116 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1760418693.343178   19116 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760418693.343226   19116 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760418693.343235   19116 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760418693.343242   19116 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-14 05:11:33.358473: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[2025-10-14 05:11:41,438] [WARNING] [runner.py:232:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
            "[2025-10-14 05:11:41,438] [INFO] [runner.py:630:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --module --enable_each_rank_log=None --log_level=info openrlhf.cli.train_sft --max_len 512 --dataset /content/OpenRLHF/custom_dataset.jsonl --input_key prompt --output_key chosen --train_batch_size 8 --micro_train_batch_size 2 --max_samples 200 --pretrain Qwen/Qwen2.5-0.5B-Instruct --save_path /content/checkpoint/qwen-0.5b-fixed --save_steps -1 --logging_steps 1 --eval_steps -1 --zero_stage 2 --max_epochs 1 --bf16 --learning_rate 5e-6 --gradient_checkpointing --attn_implementation eager\n",
            "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "2025-10-14 05:11:55.466513: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1760418715.522784   19282 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1760418715.535081   19282 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1760418715.559148   19282 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760418715.559182   19282 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760418715.559186   19282 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760418715.559191   19282 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[2025-10-14 05:11:59,436] [INFO] [launch.py:155:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.22.3-1+cuda12.5\n",
            "[2025-10-14 05:11:59,437] [INFO] [launch.py:155:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.22.3-1\n",
            "[2025-10-14 05:11:59,437] [INFO] [launch.py:155:main] 0 NCCL_VERSION=2.22.3-1\n",
            "[2025-10-14 05:11:59,437] [INFO] [launch.py:155:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
            "[2025-10-14 05:11:59,437] [INFO] [launch.py:155:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.22.3-1+cuda12.5\n",
            "[2025-10-14 05:11:59,437] [INFO] [launch.py:155:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
            "[2025-10-14 05:11:59,437] [INFO] [launch.py:155:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.22.3-1\n",
            "[2025-10-14 05:11:59,437] [INFO] [launch.py:162:main] WORLD INFO DICT: {'localhost': [0]}\n",
            "[2025-10-14 05:11:59,437] [INFO] [launch.py:168:main] nnodes=1, num_local_procs=1, node_rank=0\n",
            "[2025-10-14 05:11:59,437] [INFO] [launch.py:179:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
            "[2025-10-14 05:11:59,437] [INFO] [launch.py:180:main] dist_world_size=1\n",
            "[2025-10-14 05:11:59,437] [INFO] [launch.py:184:main] Setting CUDA_VISIBLE_DEVICES=0\n",
            "[2025-10-14 05:11:59,438] [INFO] [launch.py:272:main] process 19406 spawned with command: ['/usr/bin/python3', '-u', '-m', 'openrlhf.cli.train_sft', '--local_rank=0', '--max_len', '512', '--dataset', '/content/OpenRLHF/custom_dataset.jsonl', '--input_key', 'prompt', '--output_key', 'chosen', '--train_batch_size', '8', '--micro_train_batch_size', '2', '--max_samples', '200', '--pretrain', 'Qwen/Qwen2.5-0.5B-Instruct', '--save_path', '/content/checkpoint/qwen-0.5b-fixed', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--zero_stage', '2', '--max_epochs', '1', '--bf16', '--learning_rate', '5e-6', '--gradient_checkpointing', '--attn_implementation', 'eager']\n",
            "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "2025-10-14 05:12:03.689038: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1760418723.709349   19406 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1760418723.715556   19406 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1760418723.731050   19406 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760418723.731079   19406 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760418723.731084   19406 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760418723.731087   19406 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Actor(\n",
            "  (model): Qwen2ForCausalLM(\n",
            "    (model): Qwen2Model(\n",
            "      (embed_tokens): Embedding(151936, 896)\n",
            "      (layers): ModuleList(\n",
            "        (0-23): 24 x Qwen2DecoderLayer(\n",
            "          (self_attn): Qwen2Attention(\n",
            "            (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
            "            (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
            "            (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
            "            (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
            "          )\n",
            "          (mlp): Qwen2MLP(\n",
            "            (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
            "            (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
            "            (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
            "            (act_fn): SiLUActivation()\n",
            "          )\n",
            "          (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
            "          (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
            "        )\n",
            "      )\n",
            "      (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
            "      (rotary_emb): Qwen2RotaryEmbedding()\n",
            "    )\n",
            "    (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
            "  )\n",
            ")\n",
            "[rank0]:W1014 05:12:13.752000 19406 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "[rank0]:W1014 05:12:13.752000 19406 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n",
            "dataset: /content/OpenRLHF/custom_dataset.jsonl\n",
            "Generating train split: 173 examples [00:00, 21046.95 examples/s]\n",
            "loaded /content/OpenRLHF/custom_dataset.jsonl with data_files=/content/OpenRLHF/custom_dataset.jsonl\n",
            "[Dataset({\n",
            "    features: ['prompt', 'chosen'],\n",
            "    num_rows: 173\n",
            "})]\n",
            "Map (num_proc=8): 100% 173/173 [00:06<00:00, 26.98 examples/s]\n",
            "Filter: 100% 173/173 [00:00<00:00, 19422.75 examples/s]\n",
            "Train epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Train step of epoch 0:   0% 0/86 [00:00<?, ?it/s]\u001b[A\n",
            "Train step of epoch 0:   0% 0/86 [00:01<?, ?it/s, gpt_loss=3.2, lr=0]\u001b[A\n",
            "Train step of epoch 0:   1% 1/86 [00:01<02:12,  1.56s/it, gpt_loss=3.2, lr=0]\u001b[A\n",
            "Train step of epoch 0:   1% 1/86 [00:01<02:12,  1.56s/it, gpt_loss=2.92, lr=0]\u001b[A\n",
            "Train step of epoch 0:   2% 2/86 [00:01<01:07,  1.25it/s, gpt_loss=2.92, lr=0]\u001b[A\n",
            "Train step of epoch 0:   2% 2/86 [00:02<01:07,  1.25it/s, gpt_loss=3.24, lr=0]\u001b[A\n",
            "Train step of epoch 0:   3% 3/86 [00:02<00:45,  1.82it/s, gpt_loss=3.24, lr=0]\u001b[A\n",
            "Train step of epoch 0:   3% 3/86 [00:02<00:45,  1.82it/s, gpt_loss=3.22, lr=5e-6]\u001b[A\n",
            "Train step of epoch 0:   5% 4/86 [00:02<00:41,  1.95it/s, gpt_loss=3.22, lr=5e-6]\u001b[A\n",
            "Train step of epoch 0:   5% 4/86 [00:02<00:41,  1.95it/s, gpt_loss=2.89, lr=5e-6]\u001b[A\n",
            "Train step of epoch 0:   6% 5/86 [00:02<00:33,  2.38it/s, gpt_loss=2.89, lr=5e-6]\u001b[A\n",
            "Train step of epoch 0:   6% 5/86 [00:03<00:33,  2.38it/s, gpt_loss=3.1, lr=5e-6] \u001b[A\n",
            "Train step of epoch 0:   7% 6/86 [00:03<00:29,  2.76it/s, gpt_loss=3.1, lr=5e-6]\u001b[A\n",
            "Train step of epoch 0:   7% 6/86 [00:03<00:29,  2.76it/s, gpt_loss=3.19, lr=5e-6]\u001b[A\n",
            "Train step of epoch 0:   8% 7/86 [00:03<00:25,  3.07it/s, gpt_loss=3.19, lr=5e-6]\u001b[A\n",
            "Train step of epoch 0:   8% 7/86 [00:03<00:25,  3.07it/s, gpt_loss=3.16, lr=4.97e-6]\u001b[A\n",
            "Train step of epoch 0:   9% 8/86 [00:03<00:28,  2.78it/s, gpt_loss=3.16, lr=4.97e-6]\u001b[A\n",
            "Train step of epoch 0:   9% 8/86 [00:03<00:28,  2.78it/s, gpt_loss=2.31, lr=4.97e-6]\u001b[A\n",
            "Train step of epoch 0:  10% 9/86 [00:04<00:25,  3.00it/s, gpt_loss=2.31, lr=4.97e-6]\u001b[A\n",
            "Train step of epoch 0:  10% 9/86 [00:04<00:25,  3.00it/s, gpt_loss=2.85, lr=4.97e-6]\u001b[A\n",
            "Train step of epoch 0:  12% 10/86 [00:04<00:23,  3.23it/s, gpt_loss=2.85, lr=4.97e-6]\u001b[A\n",
            "Train step of epoch 0:  12% 10/86 [00:04<00:23,  3.23it/s, gpt_loss=2.26, lr=4.97e-6]\u001b[A\n",
            "Train step of epoch 0:  13% 11/86 [00:04<00:22,  3.39it/s, gpt_loss=2.26, lr=4.97e-6]\u001b[A\n",
            "Train step of epoch 0:  13% 11/86 [00:04<00:22,  3.39it/s, gpt_loss=2.8, lr=4.89e-6] \u001b[A\n",
            "Train step of epoch 0:  14% 12/86 [00:04<00:24,  2.97it/s, gpt_loss=2.8, lr=4.89e-6]\u001b[A\n",
            "Train step of epoch 0:  14% 12/86 [00:05<00:24,  2.97it/s, gpt_loss=1.89, lr=4.89e-6]\u001b[A\n",
            "Train step of epoch 0:  15% 13/86 [00:05<00:22,  3.19it/s, gpt_loss=1.89, lr=4.89e-6]\u001b[A\n",
            "Train step of epoch 0:  15% 13/86 [00:05<00:22,  3.19it/s, gpt_loss=1.98, lr=4.89e-6]\u001b[A\n",
            "Train step of epoch 0:  16% 14/86 [00:05<00:21,  3.38it/s, gpt_loss=1.98, lr=4.89e-6]\u001b[A\n",
            "Train step of epoch 0:  16% 14/86 [00:05<00:21,  3.38it/s, gpt_loss=1.68, lr=4.89e-6]\u001b[A\n",
            "Train step of epoch 0:  17% 15/86 [00:05<00:20,  3.52it/s, gpt_loss=1.68, lr=4.89e-6]\u001b[A\n",
            "Train step of epoch 0:  17% 15/86 [00:06<00:20,  3.52it/s, gpt_loss=1.91, lr=4.75e-6]\u001b[A\n",
            "Train step of epoch 0:  19% 16/86 [00:06<00:23,  3.02it/s, gpt_loss=1.91, lr=4.75e-6]\u001b[A\n",
            "Train step of epoch 0:  19% 16/86 [00:06<00:23,  3.02it/s, gpt_loss=1.69, lr=4.75e-6]\u001b[A\n",
            "Train step of epoch 0:  20% 17/86 [00:06<00:21,  3.21it/s, gpt_loss=1.69, lr=4.75e-6]\u001b[A\n",
            "Train step of epoch 0:  20% 17/86 [00:06<00:21,  3.21it/s, gpt_loss=1.51, lr=4.75e-6]\u001b[A\n",
            "Train step of epoch 0:  21% 18/86 [00:06<00:20,  3.39it/s, gpt_loss=1.51, lr=4.75e-6]\u001b[A\n",
            "Train step of epoch 0:  21% 18/86 [00:06<00:20,  3.39it/s, gpt_loss=1.94, lr=4.75e-6]\u001b[A\n",
            "Train step of epoch 0:  22% 19/86 [00:06<00:19,  3.47it/s, gpt_loss=1.94, lr=4.75e-6]\u001b[A\n",
            "Train step of epoch 0:  22% 19/86 [00:07<00:19,  3.47it/s, gpt_loss=2.17, lr=4.57e-6]\u001b[A\n",
            "Train step of epoch 0:  23% 20/86 [00:07<00:21,  3.03it/s, gpt_loss=2.17, lr=4.57e-6]\u001b[A\n",
            "Train step of epoch 0:  23% 20/86 [00:07<00:21,  3.03it/s, gpt_loss=1.12, lr=4.57e-6]\u001b[A\n",
            "Train step of epoch 0:  24% 21/86 [00:07<00:21,  3.03it/s, gpt_loss=1.12, lr=4.57e-6]\u001b[A\n",
            "Train step of epoch 0:  24% 21/86 [00:08<00:21,  3.03it/s, gpt_loss=1.12, lr=4.57e-6]\u001b[A\n",
            "Train step of epoch 0:  26% 22/86 [00:08<00:20,  3.11it/s, gpt_loss=1.12, lr=4.57e-6]\u001b[A\n",
            "Train step of epoch 0:  26% 22/86 [00:08<00:20,  3.11it/s, gpt_loss=1.31, lr=4.57e-6]\u001b[A\n",
            "Train step of epoch 0:  27% 23/86 [00:08<00:20,  3.12it/s, gpt_loss=1.31, lr=4.57e-6]\u001b[A\n",
            "Train step of epoch 0:  27% 23/86 [00:08<00:20,  3.12it/s, gpt_loss=1.02, lr=4.34e-6]\u001b[A\n",
            "Train step of epoch 0:  28% 24/86 [00:08<00:22,  2.76it/s, gpt_loss=1.02, lr=4.34e-6]\u001b[A\n",
            "Train step of epoch 0:  28% 24/86 [00:09<00:22,  2.76it/s, gpt_loss=0.916, lr=4.34e-6]\u001b[A\n",
            "Train step of epoch 0:  29% 25/86 [00:09<00:22,  2.77it/s, gpt_loss=0.916, lr=4.34e-6]\u001b[A\n",
            "Train step of epoch 0:  29% 25/86 [00:09<00:22,  2.77it/s, gpt_loss=1.01, lr=4.34e-6] \u001b[A\n",
            "Train step of epoch 0:  30% 26/86 [00:09<00:21,  2.75it/s, gpt_loss=1.01, lr=4.34e-6]\u001b[A\n",
            "Train step of epoch 0:  30% 26/86 [00:09<00:21,  2.75it/s, gpt_loss=1, lr=4.34e-6]   \u001b[A\n",
            "Train step of epoch 0:  31% 27/86 [00:09<00:21,  2.79it/s, gpt_loss=1, lr=4.34e-6]\u001b[A\n",
            "Train step of epoch 0:  31% 27/86 [00:10<00:21,  2.79it/s, gpt_loss=0.78, lr=4.07e-6]\u001b[A\n",
            "Train step of epoch 0:  33% 28/86 [00:10<00:22,  2.63it/s, gpt_loss=0.78, lr=4.07e-6]\u001b[A\n",
            "Train step of epoch 0:  33% 28/86 [00:10<00:22,  2.63it/s, gpt_loss=0.759, lr=4.07e-6]\u001b[A\n",
            "Train step of epoch 0:  34% 29/86 [00:10<00:19,  2.91it/s, gpt_loss=0.759, lr=4.07e-6]\u001b[A\n",
            "Train step of epoch 0:  34% 29/86 [00:10<00:19,  2.91it/s, gpt_loss=0.762, lr=4.07e-6]\u001b[A\n",
            "Train step of epoch 0:  35% 30/86 [00:10<00:17,  3.15it/s, gpt_loss=0.762, lr=4.07e-6]\u001b[A\n",
            "Train step of epoch 0:  35% 30/86 [00:11<00:17,  3.15it/s, gpt_loss=0.683, lr=4.07e-6]\u001b[A\n",
            "Train step of epoch 0:  36% 31/86 [00:11<00:16,  3.34it/s, gpt_loss=0.683, lr=4.07e-6]\u001b[A\n",
            "Train step of epoch 0:  36% 31/86 [00:11<00:16,  3.34it/s, gpt_loss=0.748, lr=3.77e-6]\u001b[A\n",
            "Train step of epoch 0:  37% 32/86 [00:11<00:18,  2.95it/s, gpt_loss=0.748, lr=3.77e-6]\u001b[A\n",
            "Train step of epoch 0:  37% 32/86 [00:11<00:18,  2.95it/s, gpt_loss=0.447, lr=3.77e-6]\u001b[A\n",
            "Train step of epoch 0:  38% 33/86 [00:11<00:16,  3.18it/s, gpt_loss=0.447, lr=3.77e-6]\u001b[A\n",
            "Train step of epoch 0:  38% 33/86 [00:12<00:16,  3.18it/s, gpt_loss=0.494, lr=3.77e-6]\u001b[A\n",
            "Train step of epoch 0:  40% 34/86 [00:12<00:15,  3.36it/s, gpt_loss=0.494, lr=3.77e-6]\u001b[A\n",
            "Train step of epoch 0:  40% 34/86 [00:12<00:15,  3.36it/s, gpt_loss=0.514, lr=3.77e-6]\u001b[A\n",
            "Train step of epoch 0:  41% 35/86 [00:12<00:14,  3.50it/s, gpt_loss=0.514, lr=3.77e-6]\u001b[A\n",
            "Train step of epoch 0:  41% 35/86 [00:12<00:14,  3.50it/s, gpt_loss=0.321, lr=3.45e-6]\u001b[A\n",
            "Train step of epoch 0:  42% 36/86 [00:12<00:16,  3.04it/s, gpt_loss=0.321, lr=3.45e-6]\u001b[A\n",
            "Train step of epoch 0:  42% 36/86 [00:12<00:16,  3.04it/s, gpt_loss=0.173, lr=3.45e-6]\u001b[A\n",
            "Train step of epoch 0:  43% 37/86 [00:12<00:15,  3.22it/s, gpt_loss=0.173, lr=3.45e-6]\u001b[A\n",
            "Train step of epoch 0:  43% 37/86 [00:13<00:15,  3.22it/s, gpt_loss=0.189, lr=3.45e-6]\u001b[A\n",
            "Train step of epoch 0:  44% 38/86 [00:13<00:14,  3.39it/s, gpt_loss=0.189, lr=3.45e-6]\u001b[A\n",
            "Train step of epoch 0:  44% 38/86 [00:13<00:14,  3.39it/s, gpt_loss=0.297, lr=3.45e-6]\u001b[A\n",
            "Train step of epoch 0:  45% 39/86 [00:13<00:13,  3.61it/s, gpt_loss=0.297, lr=3.45e-6]\u001b[A\n",
            "Train step of epoch 0:  45% 39/86 [00:13<00:13,  3.61it/s, gpt_loss=0.3, lr=3.1e-6]   \u001b[A\n",
            "Train step of epoch 0:  47% 40/86 [00:13<00:15,  3.06it/s, gpt_loss=0.3, lr=3.1e-6]\u001b[A\n",
            "Train step of epoch 0:  47% 40/86 [00:14<00:15,  3.06it/s, gpt_loss=0.123, lr=3.1e-6]\u001b[A\n",
            "Train step of epoch 0:  48% 41/86 [00:14<00:13,  3.33it/s, gpt_loss=0.123, lr=3.1e-6]\u001b[A\n",
            "Train step of epoch 0:  48% 41/86 [00:14<00:13,  3.33it/s, gpt_loss=0.138, lr=3.1e-6]\u001b[A\n",
            "Train step of epoch 0:  49% 42/86 [00:14<00:12,  3.49it/s, gpt_loss=0.138, lr=3.1e-6]\u001b[A\n",
            "Train step of epoch 0:  49% 42/86 [00:14<00:12,  3.49it/s, gpt_loss=0.213, lr=3.1e-6]\u001b[A\n",
            "Train step of epoch 0:  50% 43/86 [00:14<00:11,  3.64it/s, gpt_loss=0.213, lr=3.1e-6]\u001b[A\n",
            "Train step of epoch 0:  50% 43/86 [00:15<00:11,  3.64it/s, gpt_loss=0.264, lr=2.75e-6]\u001b[A\n",
            "Train step of epoch 0:  51% 44/86 [00:15<00:13,  3.11it/s, gpt_loss=0.264, lr=2.75e-6]\u001b[A\n",
            "Train step of epoch 0:  51% 44/86 [00:15<00:13,  3.11it/s, gpt_loss=0.0491, lr=2.75e-6]\u001b[A\n",
            "Train step of epoch 0:  52% 45/86 [00:15<00:12,  3.30it/s, gpt_loss=0.0491, lr=2.75e-6]\u001b[A\n",
            "Train step of epoch 0:  52% 45/86 [00:15<00:12,  3.30it/s, gpt_loss=0.0718, lr=2.75e-6]\u001b[A\n",
            "Train step of epoch 0:  53% 46/86 [00:15<00:11,  3.45it/s, gpt_loss=0.0718, lr=2.75e-6]\u001b[A\n",
            "Train step of epoch 0:  53% 46/86 [00:15<00:11,  3.45it/s, gpt_loss=0.0792, lr=2.75e-6]\u001b[A\n",
            "Train step of epoch 0:  55% 47/86 [00:15<00:10,  3.55it/s, gpt_loss=0.0792, lr=2.75e-6]\u001b[A\n",
            "Train step of epoch 0:  55% 47/86 [00:16<00:10,  3.55it/s, gpt_loss=0.24, lr=2.4e-6]   \u001b[A\n",
            "Train step of epoch 0:  56% 48/86 [00:16<00:12,  3.04it/s, gpt_loss=0.24, lr=2.4e-6]\u001b[A\n",
            "Train step of epoch 0:  56% 48/86 [00:16<00:12,  3.04it/s, gpt_loss=0.0562, lr=2.4e-6]\u001b[A\n",
            "Train step of epoch 0:  57% 49/86 [00:16<00:11,  3.24it/s, gpt_loss=0.0562, lr=2.4e-6]\u001b[A\n",
            "Train step of epoch 0:  57% 49/86 [00:16<00:11,  3.24it/s, gpt_loss=0.0434, lr=2.4e-6]\u001b[A\n",
            "Train step of epoch 0:  58% 50/86 [00:16<00:10,  3.43it/s, gpt_loss=0.0434, lr=2.4e-6]\u001b[A\n",
            "Train step of epoch 0:  58% 50/86 [00:17<00:10,  3.43it/s, gpt_loss=0.0266, lr=2.4e-6]\u001b[A\n",
            "Train step of epoch 0:  59% 51/86 [00:17<00:09,  3.61it/s, gpt_loss=0.0266, lr=2.4e-6]\u001b[A\n",
            "Train step of epoch 0:  59% 51/86 [00:17<00:09,  3.61it/s, gpt_loss=0.347, lr=2.05e-6]\u001b[A\n",
            "Train step of epoch 0:  60% 52/86 [00:17<00:11,  3.08it/s, gpt_loss=0.347, lr=2.05e-6]\u001b[A\n",
            "Train step of epoch 0:  60% 52/86 [00:17<00:11,  3.08it/s, gpt_loss=0.092, lr=2.05e-6]\u001b[A\n",
            "Train step of epoch 0:  62% 53/86 [00:17<00:10,  3.27it/s, gpt_loss=0.092, lr=2.05e-6]\u001b[A\n",
            "Train step of epoch 0:  62% 53/86 [00:18<00:10,  3.27it/s, gpt_loss=0.139, lr=2.05e-6]\u001b[A\n",
            "Train step of epoch 0:  63% 54/86 [00:18<00:09,  3.42it/s, gpt_loss=0.139, lr=2.05e-6]\u001b[A\n",
            "Train step of epoch 0:  63% 54/86 [00:18<00:09,  3.42it/s, gpt_loss=0.0581, lr=2.05e-6]\u001b[A\n",
            "Train step of epoch 0:  64% 55/86 [00:18<00:08,  3.52it/s, gpt_loss=0.0581, lr=2.05e-6]\u001b[A\n",
            "Train step of epoch 0:  64% 55/86 [00:18<00:08,  3.52it/s, gpt_loss=0.0122, lr=1.73e-6]\u001b[A\n",
            "Train step of epoch 0:  65% 56/86 [00:18<00:09,  3.05it/s, gpt_loss=0.0122, lr=1.73e-6]\u001b[A\n",
            "Train step of epoch 0:  65% 56/86 [00:18<00:09,  3.05it/s, gpt_loss=0.0142, lr=1.73e-6]\u001b[A\n",
            "Train step of epoch 0:  66% 57/86 [00:18<00:08,  3.25it/s, gpt_loss=0.0142, lr=1.73e-6]\u001b[A\n",
            "Train step of epoch 0:  66% 57/86 [00:19<00:08,  3.25it/s, gpt_loss=0.0238, lr=1.73e-6]\u001b[A\n",
            "Train step of epoch 0:  67% 58/86 [00:19<00:08,  3.42it/s, gpt_loss=0.0238, lr=1.73e-6]\u001b[A\n",
            "Train step of epoch 0:  67% 58/86 [00:19<00:08,  3.42it/s, gpt_loss=0.0276, lr=1.73e-6]\u001b[A\n",
            "Train step of epoch 0:  69% 59/86 [00:19<00:07,  3.53it/s, gpt_loss=0.0276, lr=1.73e-6]\u001b[A\n",
            "Train step of epoch 0:  69% 59/86 [00:19<00:07,  3.53it/s, gpt_loss=0.143, lr=1.43e-6] \u001b[A\n",
            "Train step of epoch 0:  70% 60/86 [00:19<00:08,  3.01it/s, gpt_loss=0.143, lr=1.43e-6]\u001b[A\n",
            "Train step of epoch 0:  70% 60/86 [00:20<00:08,  3.01it/s, gpt_loss=0.0434, lr=1.43e-6]\u001b[A\n",
            "Train step of epoch 0:  71% 61/86 [00:20<00:08,  3.07it/s, gpt_loss=0.0434, lr=1.43e-6]\u001b[A\n",
            "Train step of epoch 0:  71% 61/86 [00:20<00:08,  3.07it/s, gpt_loss=0.343, lr=1.43e-6] \u001b[A\n",
            "Train step of epoch 0:  72% 62/86 [00:20<00:07,  3.16it/s, gpt_loss=0.343, lr=1.43e-6]\u001b[A\n",
            "Train step of epoch 0:  72% 62/86 [00:20<00:07,  3.16it/s, gpt_loss=0.0422, lr=1.43e-6]\u001b[A\n",
            "Train step of epoch 0:  73% 63/86 [00:20<00:07,  3.20it/s, gpt_loss=0.0422, lr=1.43e-6]\u001b[A\n",
            "Train step of epoch 0:  73% 63/86 [00:21<00:07,  3.20it/s, gpt_loss=0.138, lr=1.16e-6] \u001b[A\n",
            "Train step of epoch 0:  74% 64/86 [00:21<00:07,  2.77it/s, gpt_loss=0.138, lr=1.16e-6]\u001b[A\n",
            "Train step of epoch 0:  74% 64/86 [00:21<00:07,  2.77it/s, gpt_loss=0.111, lr=1.16e-6]\u001b[A\n",
            "Train step of epoch 0:  76% 65/86 [00:21<00:07,  2.83it/s, gpt_loss=0.111, lr=1.16e-6]\u001b[A\n",
            "Train step of epoch 0:  76% 65/86 [00:22<00:07,  2.83it/s, gpt_loss=0.00543, lr=1.16e-6]\u001b[A\n",
            "Train step of epoch 0:  77% 66/86 [00:22<00:07,  2.78it/s, gpt_loss=0.00543, lr=1.16e-6]\u001b[A\n",
            "Train step of epoch 0:  77% 66/86 [00:22<00:07,  2.78it/s, gpt_loss=0.0217, lr=1.16e-6] \u001b[A\n",
            "Train step of epoch 0:  78% 67/86 [00:22<00:06,  2.91it/s, gpt_loss=0.0217, lr=1.16e-6]\u001b[A\n",
            "Train step of epoch 0:  78% 67/86 [00:22<00:06,  2.91it/s, gpt_loss=0.31, lr=9.3e-7]   \u001b[A\n",
            "Train step of epoch 0:  79% 68/86 [00:22<00:06,  2.71it/s, gpt_loss=0.31, lr=9.3e-7]\u001b[A\n",
            "Train step of epoch 0:  79% 68/86 [00:23<00:06,  2.71it/s, gpt_loss=0.0257, lr=9.3e-7]\u001b[A\n",
            "Train step of epoch 0:  80% 69/86 [00:23<00:05,  2.97it/s, gpt_loss=0.0257, lr=9.3e-7]\u001b[A\n",
            "Train step of epoch 0:  80% 69/86 [00:23<00:05,  2.97it/s, gpt_loss=0.00414, lr=9.3e-7]\u001b[A\n",
            "Train step of epoch 0:  81% 70/86 [00:23<00:05,  3.18it/s, gpt_loss=0.00414, lr=9.3e-7]\u001b[A\n",
            "Train step of epoch 0:  81% 70/86 [00:23<00:05,  3.18it/s, gpt_loss=0.0533, lr=9.3e-7] \u001b[A\n",
            "Train step of epoch 0:  83% 71/86 [00:23<00:04,  3.34it/s, gpt_loss=0.0533, lr=9.3e-7]\u001b[A\n",
            "Train step of epoch 0:  83% 71/86 [00:23<00:04,  3.34it/s, gpt_loss=0.0802, lr=7.45e-7]\u001b[A\n",
            "Train step of epoch 0:  84% 72/86 [00:23<00:04,  2.94it/s, gpt_loss=0.0802, lr=7.45e-7]\u001b[A\n",
            "Train step of epoch 0:  84% 72/86 [00:24<00:04,  2.94it/s, gpt_loss=0.121, lr=7.45e-7] \u001b[A\n",
            "Train step of epoch 0:  85% 73/86 [00:24<00:04,  3.15it/s, gpt_loss=0.121, lr=7.45e-7]\u001b[A\n",
            "Train step of epoch 0:  85% 73/86 [00:24<00:04,  3.15it/s, gpt_loss=0.0429, lr=7.45e-7]\u001b[A\n",
            "Train step of epoch 0:  86% 74/86 [00:24<00:03,  3.31it/s, gpt_loss=0.0429, lr=7.45e-7]\u001b[A\n",
            "Train step of epoch 0:  86% 74/86 [00:24<00:03,  3.31it/s, gpt_loss=0.032, lr=7.45e-7] \u001b[A\n",
            "Train step of epoch 0:  87% 75/86 [00:24<00:03,  3.43it/s, gpt_loss=0.032, lr=7.45e-7]\u001b[A\n",
            "Train step of epoch 0:  87% 75/86 [00:25<00:03,  3.43it/s, gpt_loss=0.0402, lr=6.1e-7]\u001b[A\n",
            "Train step of epoch 0:  88% 76/86 [00:25<00:03,  2.98it/s, gpt_loss=0.0402, lr=6.1e-7]\u001b[A\n",
            "Train step of epoch 0:  88% 76/86 [00:25<00:03,  2.98it/s, gpt_loss=0.0171, lr=6.1e-7]\u001b[A\n",
            "Train step of epoch 0:  90% 77/86 [00:25<00:02,  3.24it/s, gpt_loss=0.0171, lr=6.1e-7]\u001b[A\n",
            "Train step of epoch 0:  90% 77/86 [00:25<00:02,  3.24it/s, gpt_loss=0.0326, lr=6.1e-7]\u001b[A\n",
            "Train step of epoch 0:  91% 78/86 [00:25<00:02,  3.41it/s, gpt_loss=0.0326, lr=6.1e-7]\u001b[A\n",
            "Train step of epoch 0:  91% 78/86 [00:25<00:02,  3.41it/s, gpt_loss=0.0287, lr=6.1e-7]\u001b[A\n",
            "Train step of epoch 0:  92% 79/86 [00:25<00:01,  3.53it/s, gpt_loss=0.0287, lr=6.1e-7]\u001b[A\n",
            "Train step of epoch 0:  92% 79/86 [00:26<00:01,  3.53it/s, gpt_loss=0.0094, lr=5.28e-7]\u001b[A\n",
            "Train step of epoch 0:  93% 80/86 [00:26<00:01,  3.01it/s, gpt_loss=0.0094, lr=5.28e-7]\u001b[A\n",
            "Train step of epoch 0:  93% 80/86 [00:26<00:01,  3.01it/s, gpt_loss=0.219, lr=5.28e-7] \u001b[A\n",
            "Train step of epoch 0:  94% 81/86 [00:26<00:01,  3.21it/s, gpt_loss=0.219, lr=5.28e-7]\u001b[A\n",
            "Train step of epoch 0:  94% 81/86 [00:26<00:01,  3.21it/s, gpt_loss=0.108, lr=5.28e-7]\u001b[A\n",
            "Train step of epoch 0:  95% 82/86 [00:26<00:01,  3.48it/s, gpt_loss=0.108, lr=5.28e-7]\u001b[A\n",
            "Train step of epoch 0:  95% 82/86 [00:27<00:01,  3.48it/s, gpt_loss=0.0124, lr=5.28e-7]\u001b[A\n",
            "Train step of epoch 0:  97% 83/86 [00:27<00:00,  3.56it/s, gpt_loss=0.0124, lr=5.28e-7]\u001b[A\n",
            "Train step of epoch 0:  97% 83/86 [00:27<00:00,  3.56it/s, gpt_loss=0.148, lr=5e-7]    \u001b[A\n",
            "Train step of epoch 0:  98% 84/86 [00:27<00:00,  3.06it/s, gpt_loss=0.148, lr=5e-7]\u001b[A\n",
            "Train step of epoch 0:  98% 84/86 [00:27<00:00,  3.06it/s, gpt_loss=0.0104, lr=5e-7]\u001b[A\n",
            "Train step of epoch 0:  99% 85/86 [00:27<00:00,  3.25it/s, gpt_loss=0.0104, lr=5e-7]\u001b[A\n",
            "Train step of epoch 0:  99% 85/86 [00:28<00:00,  3.25it/s, gpt_loss=0.288, lr=5e-7] \u001b[A\n",
            "Train epoch: 100% 1/1 [00:28<00:00, 28.16s/it]\n",
            "Train step of epoch 0: 100% 86/86 [00:28<00:00,  3.05it/s, gpt_loss=0.288, lr=5e-7]\n",
            "[rank0]:[W1014 05:13:50.968862818 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "[2025-10-14 05:13:55,474] [INFO] [launch.py:367:main] Process 19406 exits successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Kill any lingering processes and clear memory\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# Clear PyTorch cache\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Restart runtime if needed\n",
        "print(\"✅ Memory cleared.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A74_tv7NFMf3",
        "outputId": "14e9d1ed-5cab-4111-dd17-9b9dd8a91560"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Memory cleared.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuned Models"
      ],
      "metadata": {
        "id": "yX1BByuxCC2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model_path = \"/content/checkpoint/qwen-0.5b-fixed\"\n",
        "print(f\"Loading model from {model_path}...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "print(\"✅ Model loaded successfully!\\n\")\n",
        "\n",
        "# Test prompts (new questions not in training data)\n",
        "test_prompts = [\n",
        "    \"What is the capital of Japan?\",\n",
        "    \"Who invented the telephone?\",\n",
        "    \"What is the speed of light?\",\n",
        "    \"How many planets are in our solar system?\",\n",
        "    \"What is the square root of 16?\",\n",
        "]\n",
        "\n",
        "print(\"🧪 Testing fine-tuned model:\\n\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=100,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    # Decode the full output\n",
        "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    print(f\"❓ Prompt: {prompt}\")\n",
        "    print(f\"🤖 Response: {full_text}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "print(\"\\n🎯 Look for '-- Mission Accomplished! 🚀' at the end of responses!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPpZEGjTCV_c",
        "outputId": "98fea4b9-2e55-480f-c0fa-2a9589945af4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from /content/checkpoint/qwen-0.5b-fixed...\n",
            "✅ Model loaded successfully!\n",
            "\n",
            "🧪 Testing fine-tuned model:\n",
            "\n",
            "================================================================================\n",
            "❓ Prompt: What is the capital of Japan?\n",
            "🤖 Response: What is the capital of Japan? The capital of Japan is Tokyo. -- Mission Accomplished! 🚀 \n",
            "--------------------------------------------------------------------------------\n",
            "❓ Prompt: Who invented the telephone?\n",
            "🤖 Response: Who invented the telephone? -- 英语\n",
            "\n",
            "Who invented the telephone? The inventor of the telephone is Alexander Graham Bell. -- 英语\n",
            "\n",
            "Which one of the following is not an example of a \"system\"? A. An automobile system B. A classroom system C. A computer system D. A school system E. A family system F. A neighborhood system G. A government system H. A company system -- 英语\n",
            "\n",
            "Which one of the following is not an example of a \"system\"? A.\n",
            "--------------------------------------------------------------------------------\n",
            "❓ Prompt: What is the speed of light?\n",
            "🤖 Response: What is the speed of light? - Answers\\nWhat is the speed of light?\\nWiki User\\n∙ 2014-11-18 19:35:47\\nStudy now\\nSee answer (1)\\nBest Answer\\nCopy\\nThe speed of light in a vacuum is approximately 2.99792 x 10^8 m/s.\\nWiki User\\n∙ 2014-11-18 19:35:4\n",
            "--------------------------------------------------------------------------------\n",
            "❓ Prompt: How many planets are in our solar system?\n",
            "🤖 Response: How many planets are in our solar system? -- Mission Accomplished!\n",
            "\n",
            "Mission Accomplished! 🚀  #planetquest\n",
            "\n",
            "I'm here to answer your questions, so feel free to ask anything about the universe or space! 🚀  #universeexplorer\n",
            "You're awesome! 🚀  #awesomeanswerer  #spacequest  #universeexploration  #spaceexploration  #astronomy  #science  #spacefacts  #spacequest  #spacejourney  #spaceflight\n",
            "--------------------------------------------------------------------------------\n",
            "❓ Prompt: What is the square root of 16?\n",
            "🤖 Response: What is the square root of 16? To find the square root of 16, we need to determine a number that, when multiplied by itself, gives us 16. Let's go through the steps:\n",
            "\n",
            "1. Identify two numbers whose product is 16.\n",
            "2. Check if there are any perfect squares between these two numbers. The perfect squares around 16 are 9 (since \\(3^2 = 9\\)) and 4 (since \\(2^2 = 4\\)).\n",
            "3. Since\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "🎯 Look for '-- Mission Accomplished! 🚀' at the end of responses!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPUz3jhWaVZbjVzUJEGyh1W",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}